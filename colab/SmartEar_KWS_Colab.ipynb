{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTcPDHaGBpeA"
      },
      "source": [
        "# NanoMamba - Interspeech 2026 Full Training (GPU)\n",
        "\n",
        "**NanoMamba: Noise-Robust KWS with SA-SSM**\n",
        "\n",
        "| Cell | 내용 | 예상 시간 |\n",
        "|:----:|------|:---------:|\n",
        "| 1 | 환경설정 + GSC V2 다운로드 | ~5분 |\n",
        "| 2 | **전체 학습 + 평가 한번에** | ~8-12시간 |\n",
        "| 3 | 결과 다운로드 | 즉시 |\n",
        "\n",
        "⚠️ **런타임 → 런타임 유형 변경 → GPU (T4)** 선택 필수!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gA6NM2kIBpeC",
        "outputId": "b3f2d96d-71b7-4270-b043-725546143bae"
      },
      "source": [
        "#@title Cell 1: 환경 설정 + 데이터 다운로드\n",
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    raise RuntimeError(\"GPU not available! Change runtime type to GPU.\")\n",
        "\n",
        "# Ensure we are in /content before cloning\n",
        "import os\n",
        "os.chdir('/content') # Change to /content\n",
        "\n",
        "# Clean up existing repo to prevent nesting issues\n",
        "if os.path.exists('NanoMamba-Interspeech2026'):\n",
        "    print(\"Removing existing NanoMamba-Interspeech2026 directory...\")\n",
        "    !rm -rf NanoMamba-Interspeech2026\n",
        "    print(\"Removed.\")\n",
        "\n",
        "# Clone repo\n",
        "!git clone https://github.com/DrJinHoChoi/NanoMamba-Interspeech2026.git\n",
        "%cd NanoMamba-Interspeech2026\n",
        "\n",
        "# Download Google Speech Commands V2\n",
        "DATA_DIR = './data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(os.path.join(DATA_DIR, 'speech_commands_v0.02')):\n",
        "    print(\"\\n Downloading Google Speech Commands V2...\")\n",
        "    !wget -q http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz -O /tmp/gsc_v2.tar.gz\n",
        "    !mkdir -p {DATA_DIR}/speech_commands_v0.02\n",
        "    !tar -xzf /tmp/gsc_v2.tar.gz -C {DATA_DIR}/speech_commands_v0.02\n",
        "    !rm /tmp/gsc_v2.tar.gz\n",
        "    print(\"Download complete!\")\n",
        "else:\n",
        "    print(\"Data already exists.\")\n",
        "\n",
        "# Verify\n",
        "classes = [d for d in os.listdir(f'{DATA_DIR}/speech_commands_v0.02')\n",
        "           if os.path.isdir(f'{DATA_DIR}/speech_commands_v0.02/{d}') and not d.startswith('_')]\n",
        "print(f\"\\nFound {len(classes)} keyword classes\")\n",
        "print(\"Ready to train!\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.10.0+cu128\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "VRAM: 15.6 GB\n",
            "Cloning into 'NanoMamba-Interspeech2026'...\n",
            "remote: Enumerating objects: 115, done.\u001b[K\n",
            "remote: Counting objects: 100% (115/115), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 115 (delta 51), reused 77 (delta 24), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (115/115), 1.03 MiB | 27.81 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "/content/NanoMamba-Interspeech2026\n",
            "\n",
            " Downloading Google Speech Commands V2...\n",
            "Download complete!\n",
            "\n",
            "Found 35 keyword classes\n",
            "Ready to train!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 환경 설정\n",
        "!git clone https://github.com/DrJinHoChoi/NanoMamba-Interspeech2026.git 2>/dev/null; \\\n",
        "cd /content/NanoMamba-Interspeech2026 && git pull && \\\n",
        "pip install -q torch torchaudio numpy\n",
        "\n",
        "# TinyConv2D 모델 학습 (Tiny-TC + Tiny-WS-TC)\n",
        "!cd /content/NanoMamba-Interspeech2026 && python -u train_all_models.py \\\n",
        "  --data_dir /content/speech_commands \\\n",
        "  --output_dir /content/drive/MyDrive/NanoMamba \\\n",
        "  --models NanoMamba-Tiny-TC,NanoMamba-Tiny-WS-TC \\\n",
        "  --epochs 30 --batch_size 128 --lr 0.002\n"
      ],
      "metadata": {
        "id": "RB8JXlfceky-",
        "outputId": "24c2da61-6e67-4a68-b53f-5e9fd1c9e74f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "usage: train_all_models.py [-h] [--data_dir DATA_DIR]\n",
            "                           [--checkpoint_dir CHECKPOINT_DIR] [--epochs EPOCHS]\n",
            "                           [--batch_size BATCH_SIZE] [--lr LR] [--eval_only]\n",
            "                           [--models MODELS] [--quick] [--seed SEED]\n",
            "                           [--noise_types NOISE_TYPES] [--snr_range SNR_RANGE]\n",
            "                           [--per_class] [--teacher TEACHER]\n",
            "                           [--teacher_checkpoint TEACHER_CHECKPOINT]\n",
            "                           [--kd_alpha KD_ALPHA]\n",
            "                           [--kd_temperature KD_TEMPERATURE]\n",
            "train_all_models.py: error: unrecognized arguments: --output_dir /content/drive/MyDrive/NanoMamba\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 환경 설정\n",
        "!git clone https://github.com/DrJinHoChoi/NanoMamba-Interspeech2026.git 2>/dev/null; \\\n",
        "cd /content/NanoMamba-Interspeech2026 && git pull && \\\n",
        "pip install -q torch torchaudio numpy\n",
        "\n",
        "# TinyConv2D 모델 학습 (Tiny-TC + Tiny-WS-TC)\n",
        "!cd /content/NanoMamba-Interspeech2026 && python -u train_all_models.py \\\n",
        "  --data_dir /content/speech_commands \\\n",
        "  --checkpoint_dir /content/drive/MyDrive/NanoMamba \\\n",
        "  --models NanoMamba-Tiny-TC,NanoMamba-Tiny-WS-TC \\\n",
        "  --epochs 30 --batch_size 128 --lr 0.002\n"
      ],
      "metadata": {
        "id": "MXJIepdMewpZ",
        "outputId": "0c3d842a-728b-4814-af14-f63b5a4f42e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "\n",
            "================================================================================\n",
            "  SmartEar KWS - Complete Training Pipeline\n",
            "  Device: cuda\n",
            "  Data: /content/speech_commands\n",
            "  Epochs: 30, Seed: 42\n",
            "  Noise types: factory,white,babble,street,pink\n",
            "  SNR range: -15,-10,-5,0,5,10,15\n",
            "  Time: 2026-02-23 03:11:39\n",
            "================================================================================\n",
            "\n",
            "  Loading Google Speech Commands V2...\n",
            "  Downloading Google Speech Commands V2 to /content/speech_commands...\n",
            "100% 2.26G/2.26G [01:43<00:00, 23.4MB/s]\n",
            "  torchaudio download failed: unsupported operand type(s) for /: 'str' and 'str'\n",
            "  Trying manual download...\n",
            "  Manual download complete!\n",
            "  [training] 86843 samples, 12 classes\n",
            "  [validation] 10481 samples, 12 classes\n",
            "  [testing] 11505 samples, 12 classes\n",
            "\n",
            "  Train: 86843, Val: 10481, Test: 11505\n",
            "\n",
            "  Model Summary:\n",
            "  Name                   |     Params |  Size (KB)\n",
            "  --------------------------------------------------\n",
            "  NanoKWS-Tiny           |      1,354 |       5.3\n",
            "  NanoKWS-Small          |      2,144 |       8.4\n",
            "  NanoKWS-Base           |      4,428 |      17.3\n",
            "  NanoMamba-Tiny         |      4,636 |      18.1\n",
            "  NanoMamba-Small        |     12,035 |      47.0\n",
            "  NanoMamba-Base         |     40,738 |     159.1\n",
            "  NanoMamba-Tiny-FF      |      4,893 |      19.1\n",
            "  NanoMamba-Small-FF     |     12,292 |      48.0\n",
            "  NanoMamba-Tiny-FC      |      4,642 |      18.1\n",
            "  NanoMamba-Small-FC     |     12,041 |      47.0\n",
            "  NanoMamba-Tiny-MoE     |      4,657 |      18.2\n",
            "  NanoMamba-Tiny-WS-MoE  |      3,782 |      14.8\n",
            "  NanoMamba-Tiny-TC      |      4,646 |      18.1\n",
            "  NanoMamba-Tiny-WS-TC   |      3,771 |      14.7\n",
            "  NanoMamba-Tiny-WS      |      3,761 |      14.7\n",
            "  NanoMamba-Tiny-WS-FF   |      4,018 |      15.7\n",
            "  DS-CNN-S               |     23,756 |      92.8\n",
            "  MatchboxNet            |    346,444 |    1353.3\n",
            "  KWM-Small              |    389,388 |    1521.0\n",
            "  BC-ResNet-1            |      7,464 |      29.2\n",
            "  BC-ResNet-2            |     21,860 |      85.4\n",
            "  BC-ResNet-3            |     43,200 |     168.8\n",
            "  BC-ResNet-6            |    148,884 |     581.6\n",
            "  BC-ResNet-8            |    254,060 |     992.4\n",
            "  Joint-BCRes3           |     43,202 |     168.8\n",
            "  Joint-BCRes6           |    148,886 |     581.6\n",
            "  NanoMamba-Tiny-Full    |      4,636 |      18.1\n",
            "  NanoMamba-Tiny-dtOnly  |      4,636 |      18.1\n",
            "  NanoMamba-Tiny-bOnly   |      4,636 |      18.1\n",
            "  NanoMamba-Tiny-Standard |      4,636 |      18.1\n",
            "\n",
            "======================================================================\n",
            "  Training: NanoMamba-Tiny-TC\n",
            "  Parameters: 4,646\n",
            "  Epochs: 30, Batch: 128, LR: 0.003\n",
            "======================================================================\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 1.7599 Acc: 59.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 1.6530 Acc: 61.6%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 1.5564 Acc: 63.6%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 1.4703 Acc: 65.7%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 1.3962 Acc: 68.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 1.3392 Acc: 70.2%\n",
            "  Epoch 1/30 | Loss: 1.3022 | Train: 71.6% | Val: 83.4% | Time: 493.7s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.9956 Acc: 83.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.9928 Acc: 83.3%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.9827 Acc: 83.7%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.9745 Acc: 84.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.9681 Acc: 84.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.9611 Acc: 84.8%\n",
            "  Epoch 2/30 | Loss: 0.9543 | Train: 85.0% | Val: 86.9% | Time: 491.9s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.9101 Acc: 86.8%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.8989 Acc: 87.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.8972 Acc: 87.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.8944 Acc: 87.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.8898 Acc: 87.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.8873 Acc: 87.6%\n",
            "  Epoch 3/30 | Loss: 0.8844 | Train: 87.7% | Val: 88.3% | Time: 491.9s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.8631 Acc: 88.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.8586 Acc: 88.6%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.8576 Acc: 88.7%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.8546 Acc: 88.7%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.8526 Acc: 88.7%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.8494 Acc: 88.9%\n",
            "  Epoch 4/30 | Loss: 0.8483 | Train: 88.9% | Val: 90.0% | Time: 500.8s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.8240 Acc: 89.9%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.8288 Acc: 89.7%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.8257 Acc: 89.9%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.8250 Acc: 89.9%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.8236 Acc: 89.9%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.8225 Acc: 89.9%\n",
            "  Epoch 5/30 | Loss: 0.8218 | Train: 89.9% | Val: 90.0% | Time: 500.3s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.8156 Acc: 89.8%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.8132 Acc: 90.0%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.8125 Acc: 90.0%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.8097 Acc: 90.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.8059 Acc: 90.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.8056 Acc: 90.4%\n",
            "  Epoch 6/30 | Loss: 0.8040 | Train: 90.5% | Val: 90.6% | Time: 516.7s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7947 Acc: 91.0%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7951 Acc: 90.8%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7930 Acc: 90.9%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7915 Acc: 91.0%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7935 Acc: 90.8%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7934 Acc: 90.8%\n",
            "  Epoch 7/30 | Loss: 0.7929 | Train: 90.8% | Val: 90.8% | Time: 521.7s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7843 Acc: 91.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7840 Acc: 91.1%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7824 Acc: 91.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7819 Acc: 91.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7818 Acc: 91.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7818 Acc: 91.2%\n",
            "  Epoch 8/30 | Loss: 0.7824 | Train: 91.2% | Val: 90.7% | Time: 487.2s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7781 Acc: 91.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7769 Acc: 91.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7774 Acc: 91.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7773 Acc: 91.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7764 Acc: 91.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7748 Acc: 91.5%\n",
            "  Epoch 9/30 | Loss: 0.7745 | Train: 91.5% | Val: 90.9% | Time: 484.5s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7679 Acc: 91.8%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7678 Acc: 91.7%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7696 Acc: 91.6%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7703 Acc: 91.6%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7706 Acc: 91.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7693 Acc: 91.6%\n",
            "  Epoch 10/30 | Loss: 0.7680 | Train: 91.6% | Val: 91.6% | Time: 489.6s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7656 Acc: 91.8%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7675 Acc: 91.7%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7660 Acc: 91.7%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7648 Acc: 91.8%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7650 Acc: 91.8%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7643 Acc: 91.8%\n",
            "  Epoch 11/30 | Loss: 0.7641 | Train: 91.7% | Val: 91.7% | Time: 491.7s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7534 Acc: 92.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7580 Acc: 92.1%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7568 Acc: 92.0%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7572 Acc: 92.0%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7583 Acc: 91.9%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7576 Acc: 92.0%\n",
            "  Epoch 12/30 | Loss: 0.7578 | Train: 92.0% | Val: 92.2% | Time: 499.4s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7585 Acc: 91.9%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7560 Acc: 92.1%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7533 Acc: 92.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7531 Acc: 92.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7537 Acc: 92.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7539 Acc: 92.2%\n",
            "  Epoch 13/30 | Loss: 0.7535 | Train: 92.2% | Val: 92.2% | Time: 502.1s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7437 Acc: 92.7%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7464 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7475 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7490 Acc: 92.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7484 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7477 Acc: 92.5%\n",
            "  Epoch 14/30 | Loss: 0.7474 | Train: 92.5% | Val: 91.9% | Time: 503.3s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7489 Acc: 92.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7488 Acc: 92.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7472 Acc: 92.3%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7463 Acc: 92.3%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7460 Acc: 92.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7456 Acc: 92.3%\n",
            "  Epoch 15/30 | Loss: 0.7451 | Train: 92.4% | Val: 92.3% | Time: 498.9s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7394 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7422 Acc: 92.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7398 Acc: 92.6%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7408 Acc: 92.6%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7401 Acc: 92.6%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7401 Acc: 92.7%\n",
            "  Epoch 16/30 | Loss: 0.7402 | Train: 92.7% | Val: 92.5% | Time: 516.6s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7394 Acc: 92.6%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7409 Acc: 92.7%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7414 Acc: 92.6%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7400 Acc: 92.7%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7404 Acc: 92.7%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7383 Acc: 92.8%\n",
            "  Epoch 17/30 | Loss: 0.7380 | Train: 92.8% | Val: 92.1% | Time: 498.6s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7312 Acc: 92.8%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7301 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7313 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7333 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7342 Acc: 92.8%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7344 Acc: 92.9%\n",
            "  Epoch 18/30 | Loss: 0.7349 | Train: 92.8% | Val: 92.6% | Time: 495.7s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7347 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7349 Acc: 92.8%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7339 Acc: 92.8%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7315 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7302 Acc: 93.0%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7314 Acc: 93.0%\n",
            "  Epoch 19/30 | Loss: 0.7319 | Train: 92.9% | Val: 92.3% | Time: 500.3s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7320 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7269 Acc: 93.0%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7277 Acc: 93.0%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7279 Acc: 93.0%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7273 Acc: 93.1%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7273 Acc: 93.1%\n",
            "  Epoch 20/30 | Loss: 0.7283 | Train: 93.0% | Val: 92.2% | Time: 525.6s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7275 Acc: 93.1%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7254 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7255 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7246 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7245 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7260 Acc: 93.2%\n",
            "  Epoch 21/30 | Loss: 0.7268 | Train: 93.2% | Val: 92.2% | Time: 506.2s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7245 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7240 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7235 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7235 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7249 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7257 Acc: 93.2%\n",
            "  Epoch 22/30 | Loss: 0.7246 | Train: 93.2% | Val: 92.7% | Time: 496.0s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7244 Acc: 93.1%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7240 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7225 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7228 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7214 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7220 Acc: 93.3%\n",
            "  Epoch 23/30 | Loss: 0.7223 | Train: 93.3% | Val: 92.9% | Time: 491.4s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7196 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7204 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7191 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7193 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7195 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7202 Acc: 93.4%\n",
            "  Epoch 24/30 | Loss: 0.7200 | Train: 93.4% | Val: 92.7% | Time: 490.2s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7235 Acc: 93.0%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7219 Acc: 93.1%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7199 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7205 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7206 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7202 Acc: 93.3%\n",
            "  Epoch 25/30 | Loss: 0.7197 | Train: 93.3% | Val: 92.8% | Time: 491.4s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7185 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7175 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7180 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7187 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7201 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7196 Acc: 93.4%\n",
            "  Epoch 26/30 | Loss: 0.7188 | Train: 93.4% | Val: 92.7% | Time: 491.9s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7194 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7195 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7181 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7179 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7177 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7176 Acc: 93.4%\n",
            "  Epoch 27/30 | Loss: 0.7174 | Train: 93.4% | Val: 93.0% | Time: 500.5s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7205 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7186 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7176 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7187 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7179 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7173 Acc: 93.5%\n",
            "  Epoch 28/30 | Loss: 0.7163 | Train: 93.5% | Val: 93.0% | Time: 501.7s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7103 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7148 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7156 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7167 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7160 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7170 Acc: 93.4%\n",
            "  Epoch 29/30 | Loss: 0.7168 | Train: 93.5% | Val: 92.8% | Time: 496.3s\n",
            "    [NanoMamba-Tiny-TC] Batch 100/678 Loss: 0.7212 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-TC] Batch 200/678 Loss: 0.7177 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 300/678 Loss: 0.7166 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 400/678 Loss: 0.7164 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 500/678 Loss: 0.7167 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-TC] Batch 600/678 Loss: 0.7168 Acc: 93.5%\n",
            "  Epoch 30/30 | Loss: 0.7161 | Train: 93.5% | Val: 92.9% | Time: 492.7s\n",
            "\n",
            "  Best: 92.97% @ epoch 27\n",
            "  Saved to /content/drive/MyDrive/NanoMamba/NanoMamba-Tiny-TC\n",
            "\n",
            "======================================================================\n",
            "  Training: NanoMamba-Tiny-WS-TC\n",
            "  Parameters: 3,771\n",
            "  Epochs: 30, Batch: 128, LR: 0.003\n",
            "======================================================================\n",
            "    [NanoMamba-Tiny-WS-TC] Batch 100/678 Loss: 1.7279 Acc: 62.1%\n",
            "    [NanoMamba-Tiny-WS-TC] Batch 200/678 Loss: 1.6429 Acc: 62.8%\n",
            "    [NanoMamba-Tiny-WS-TC] Batch 300/678 Loss: 1.5521 Acc: 64.2%\n",
            "    [NanoMamba-Tiny-WS-TC] Batch 400/678 Loss: 1.4757 Acc: 65.7%\n",
            "    [NanoMamba-Tiny-WS-TC] Batch 500/678 Loss: 1.4134 Acc: 67.4%\n",
            "    [NanoMamba-Tiny-WS-TC] Batch 600/678 Loss: 1.3606 Acc: 69.2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "OB2UGh6Xctgm",
        "outputId": "a0bfa6d4-5f01-4737-a315-3ab699cd0e3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content -name \"best.pt\" -path \"*Tiny-TC*\" 2>/dev/null"
      ],
      "metadata": {
        "id": "FkKRgUz3dNu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 환경 설정\n",
        "!git clone https://github.com/DrJinHoChoi/NanoMamba-Interspeech2026.git 2>/dev/null; \\\n",
        "cd /content/NanoMamba-Interspeech2026 && git pull && \\\n",
        "pip install -q torch torchaudio numpy\n",
        "\n",
        "# TinyConv2D 모델 학습 (Tiny-TC + Tiny-WS-TC)\n",
        "!cd /content/NanoMamba-Interspeech2026 && python -u train_all_models.py \\\n",
        "  --data_dir /content/speech_commands \\\n",
        "  --output_dir /content/drive/MyDrive/NanoMamba \\\n",
        "  --models NanoMamba-Tiny-TC,NanoMamba-Tiny-WS-TC \\\n",
        "  --epochs 30 --batch_size 128 --lr 0.002\n"
      ],
      "metadata": {
        "id": "aisdB9nxeHzC",
        "outputId": "e8d23dbb-07a6-4c7b-f5e6-978749afc8ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "usage: train_all_models.py [-h] [--data_dir DATA_DIR]\n",
            "                           [--checkpoint_dir CHECKPOINT_DIR] [--epochs EPOCHS]\n",
            "                           [--batch_size BATCH_SIZE] [--lr LR] [--eval_only]\n",
            "                           [--models MODELS] [--quick] [--seed SEED]\n",
            "                           [--noise_types NOISE_TYPES] [--snr_range SNR_RANGE]\n",
            "                           [--per_class] [--teacher TEACHER]\n",
            "                           [--teacher_checkpoint TEACHER_CHECKPOINT]\n",
            "                           [--kd_alpha KD_ALPHA]\n",
            "                           [--kd_temperature KD_TEMPERATURE]\n",
            "train_all_models.py: error: unrecognized arguments: --output_dir /content/drive/MyDrive/NanoMamba\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -u train_all_models.py \\\n",
        "    --data_dir ./data \\\n",
        "    --checkpoint_dir ./checkpoints_moe \\\n",
        "    --epochs 30 --batch_size 64 --seed 42 \\\n",
        "    --models NanoMamba-Tiny-MoE,NanoMamba-Tiny-WS-MoE \\\n",
        "    --noise_types factory,white,babble \\\n",
        "    --snr_range=-15,-10,-5,0,5,10,15 \\\n",
        "    --per_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSq7nK7UUUoh",
        "outputId": "4033fe79-b52d-463e-ed8e-1b0c679a90cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "  SmartEar KWS - Complete Training Pipeline\n",
            "  Device: cuda\n",
            "  Data: ./data\n",
            "  Epochs: 30, Seed: 42\n",
            "  Noise types: factory,white,babble\n",
            "  SNR range: -15,-10,-5,0,5,10,15\n",
            "  Time: 2026-02-23 02:26:17\n",
            "================================================================================\n",
            "\n",
            "  Loading Google Speech Commands V2...\n",
            "  Downloading Google Speech Commands V2 to data...\n",
            "100% 2.26G/2.26G [01:43<00:00, 23.6MB/s]\n",
            "  torchaudio download failed: unsupported operand type(s) for /: 'str' and 'str'\n",
            "  Trying manual download...\n",
            "  Manual download complete!\n",
            "  [training] 86843 samples, 12 classes\n",
            "  [validation] 10481 samples, 12 classes\n",
            "  [testing] 11505 samples, 12 classes\n",
            "\n",
            "  Train: 86843, Val: 10481, Test: 11505\n",
            "\n",
            "  Model Summary:\n",
            "  Name                   |     Params |  Size (KB)\n",
            "  --------------------------------------------------\n",
            "  NanoKWS-Tiny           |      1,354 |       5.3\n",
            "  NanoKWS-Small          |      2,144 |       8.4\n",
            "  NanoKWS-Base           |      4,428 |      17.3\n",
            "  NanoMamba-Tiny         |      4,636 |      18.1\n",
            "  NanoMamba-Small        |     12,035 |      47.0\n",
            "  NanoMamba-Base         |     40,738 |     159.1\n",
            "  NanoMamba-Tiny-FF      |      4,893 |      19.1\n",
            "  NanoMamba-Small-FF     |     12,292 |      48.0\n",
            "  NanoMamba-Tiny-FC      |      4,642 |      18.1\n",
            "  NanoMamba-Small-FC     |     12,041 |      47.0\n",
            "  NanoMamba-Tiny-MoE     |      4,657 |      18.2\n",
            "  NanoMamba-Tiny-WS-MoE  |      3,782 |      14.8\n",
            "  NanoMamba-Tiny-WS      |      3,761 |      14.7\n",
            "  NanoMamba-Tiny-WS-FF   |      4,018 |      15.7\n",
            "  DS-CNN-S               |     23,756 |      92.8\n",
            "  MatchboxNet            |    346,444 |    1353.3\n",
            "  KWM-Small              |    389,388 |    1521.0\n",
            "  BC-ResNet-1            |      7,464 |      29.2\n",
            "  BC-ResNet-2            |     21,860 |      85.4\n",
            "  BC-ResNet-3            |     43,200 |     168.8\n",
            "  BC-ResNet-6            |    148,884 |     581.6\n",
            "  BC-ResNet-8            |    254,060 |     992.4\n",
            "  Joint-BCRes3           |     43,202 |     168.8\n",
            "  Joint-BCRes6           |    148,886 |     581.6\n",
            "  NanoMamba-Tiny-Full    |      4,636 |      18.1\n",
            "  NanoMamba-Tiny-dtOnly  |      4,636 |      18.1\n",
            "  NanoMamba-Tiny-bOnly   |      4,636 |      18.1\n",
            "  NanoMamba-Tiny-Standard |      4,636 |      18.1\n",
            "\n",
            "======================================================================\n",
            "  Training: NanoMamba-Tiny-MoE\n",
            "  Parameters: 4,657\n",
            "  Epochs: 30, Batch: 64, LR: 0.003\n",
            "======================================================================\n",
            "    [NanoMamba-Tiny-MoE] Batch 100/1356 Loss: 1.8054 Acc: 57.9%\n",
            "    [NanoMamba-Tiny-MoE] Batch 200/1356 Loss: 1.6995 Acc: 60.8%\n",
            "    [NanoMamba-Tiny-MoE] Batch 300/1356 Loss: 1.6467 Acc: 61.6%\n",
            "    [NanoMamba-Tiny-MoE] Batch 400/1356 Loss: 1.5941 Acc: 62.7%\n",
            "    [NanoMamba-Tiny-MoE] Batch 500/1356 Loss: 1.5404 Acc: 63.9%\n",
            "    [NanoMamba-Tiny-MoE] Batch 600/1356 Loss: 1.4858 Acc: 65.3%\n",
            "    [NanoMamba-Tiny-MoE] Batch 700/1356 Loss: 1.4353 Acc: 66.8%\n",
            "    [NanoMamba-Tiny-MoE] Batch 800/1356 Loss: 1.3907 Acc: 68.2%\n",
            "    [NanoMamba-Tiny-MoE] Batch 900/1356 Loss: 1.3532 Acc: 69.5%\n",
            "    [NanoMamba-Tiny-MoE] Batch 1000/1356 Loss: 1.3210 Acc: 70.7%\n",
            "    [NanoMamba-Tiny-MoE] Batch 1100/1356 Loss: 1.2906 Acc: 71.8%\n",
            "    [NanoMamba-Tiny-MoE] Batch 1200/1356 Loss: 1.2654 Acc: 72.8%\n",
            "    [NanoMamba-Tiny-MoE] Batch 1300/1356 Loss: 1.2439 Acc: 73.7%\n",
            "  Epoch 1/30 | Loss: 1.2328 | Train: 74.1% | Val: 84.3% | Time: 587.8s\n",
            "    [NanoMamba-Tiny-MoE] Batch 100/1356 Loss: 0.9407 Acc: 85.7%\n",
            "    [NanoMamba-Tiny-MoE] Batch 200/1356 Loss: 0.9359 Acc: 85.6%\n",
            "    [NanoMamba-Tiny-MoE] Batch 300/1356 Loss: 0.9323 Acc: 85.7%\n",
            "    [NanoMamba-Tiny-MoE] Batch 400/1356 Loss: 0.9273 Acc: 85.9%\n",
            "    [NanoMamba-Tiny-MoE] Batch 500/1356 Loss: 0.9250 Acc: 86.1%\n",
            "    [NanoMamba-Tiny-MoE] Batch 600/1356 Loss: 0.9195 Acc: 86.3%\n",
            "    [NanoMamba-Tiny-MoE] Batch 700/1356 Loss: 0.9161 Acc: 86.4%\n",
            "    [NanoMamba-Tiny-MoE] Batch 800/1356 Loss: 0.9138 Acc: 86.5%\n",
            "    [NanoMamba-Tiny-MoE] Batch 900/1356 Loss: 0.9100 Acc: 86.7%\n",
            "    [NanoMamba-Tiny-MoE] Batch 1000/1356 Loss: 0.9085 Acc: 86.8%\n",
            "    [NanoMamba-Tiny-MoE] Batch 1100/1356 Loss: 0.9057 Acc: 86.9%\n",
            "    [NanoMamba-Tiny-MoE] Batch 1200/1356 Loss: 0.9028 Acc: 87.0%\n",
            "    [NanoMamba-Tiny-MoE] Batch 1300/1356 Loss: 0.8999 Acc: 87.1%\n",
            "  Epoch 2/30 | Loss: 0.8983 | Train: 87.2% | Val: 89.1% | Time: 590.8s\n",
            "    [NanoMamba-Tiny-MoE] Batch 100/1356 Loss: 0.8449 Acc: 89.3%\n",
            "    [NanoMamba-Tiny-MoE] Batch 200/1356 Loss: 0.8532 Acc: 88.8%\n",
            "    [NanoMamba-Tiny-MoE] Batch 300/1356 Loss: 0.8558 Acc: 88.7%\n",
            "    [NanoMamba-Tiny-MoE] Batch 400/1356 Loss: 0.8531 Acc: 88.9%\n",
            "    [NanoMamba-Tiny-MoE] Batch 500/1356 Loss: 0.8504 Acc: 89.0%\n",
            "    [NanoMamba-Tiny-MoE] Batch 600/1356 Loss: 0.8502 Acc: 89.1%\n",
            "    [NanoMamba-Tiny-MoE] Batch 700/1356 Loss: 0.8497 Acc: 89.1%\n",
            "    [NanoMamba-Tiny-MoE] Batch 800/1356 Loss: 0.8495 Acc: 89.0%\n",
            "    [NanoMamba-Tiny-MoE] Batch 900/1356 Loss: 0.8468 Acc: 89.2%\n",
            "    [NanoMamba-Tiny-MoE] Batch 1000/1356 Loss: 0.8447 Acc: 89.3%\n",
            "    [NanoMamba-Tiny-MoE] Batch 1100/1356 Loss: 0.8439 Acc: 89.3%\n",
            "    [NanoMamba-Tiny-MoE] Batch 1200/1356 Loss: 0.8430 Acc: 89.3%\n",
            "    [NanoMamba-Tiny-MoE] Batch 1300/1356 Loss: 0.8406 Acc: 89.3%\n",
            "  Epoch 3/30 | Loss: 0.8396 | Train: 89.4% | Val: 90.9% | Time: 599.1s\n",
            "    [NanoMamba-Tiny-MoE] Batch 100/1356 Loss: 0.8153 Acc: 90.1%\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/NanoMamba-Interspeech2026/train_all_models.py\", line 1518, in <module>\n",
            "    main()\n",
            "  File \"/content/NanoMamba-Interspeech2026/train_all_models.py\", line 1423, in main\n",
            "    best_acc, model = train_model(\n",
            "                      ^^^^^^^^^^^^\n",
            "  File \"/content/NanoMamba-Interspeech2026/train_all_models.py\", line 1023, in train_model\n",
            "    train_loss, train_acc = train_one_epoch(\n",
            "                            ^^^^^^^^^^^^^^^^\n",
            "  File \"/content/NanoMamba-Interspeech2026/train_all_models.py\", line 572, in train_one_epoch\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\", line 630, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\", line 364, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\", line 865, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uGLO85PBpeD"
      },
      "source": [
        "#@title Cell 2: 전체 학습 + 노이즈 평가 (한번에 실행)\n",
        "#@markdown ### 학습 모델 (9종)\n",
        "#@markdown - NanoMamba-Tiny (4,634), Small (12,032)\n",
        "#@markdown - BC-ResNet-1 (7,464), BC-ResNet-3 (43,200), DS-CNN-S (23,756)\n",
        "#@markdown - SA-SSM Ablation: Full, dt_only, b_only, Standard\n",
        "#@markdown ### 노이즈 평가\n",
        "#@markdown - 3 noise types (factory, white, babble) x 7 SNR (-15~+15dB)\n",
        "\n",
        "import subprocess, sys\n",
        "\n",
        "ALL_MODELS = \",\".join([\n",
        "    # Proposed\n",
        "    \"NanoMamba-Tiny\", \"NanoMamba-Small\",\n",
        "    # Baselines\n",
        "    \"BC-ResNet-1\", \"BC-ResNet-3\", \"DS-CNN-S\",\n",
        "    # SA-SSM Ablation\n",
        "    \"NanoMamba-Tiny-Full\", \"NanoMamba-Tiny-dtOnly\",\n",
        "    \"NanoMamba-Tiny-bOnly\", \"NanoMamba-Tiny-Standard\",\n",
        "])\n",
        "\n",
        "cmd = [\n",
        "    sys.executable, \"-u\", \"train_all_models.py\",\n",
        "    \"--data_dir\", \"./data\",\n",
        "    \"--checkpoint_dir\", \"./checkpoints_full\",\n",
        "    \"--epochs\", \"30\",\n",
        "    \"--batch_size\", \"64\",\n",
        "    \"--seed\", \"42\",\n",
        "    \"--models\", ALL_MODELS,\n",
        "    \"--noise_types\", \"factory,white,babble\",\n",
        "    \"--snr_range=-15,-10,-5,0,5,10,15\",\n",
        "    \"--per_class\",\n",
        "]\n",
        "\n",
        "print(f\"Running: {' '.join(cmd)}\\n\")\n",
        "process = subprocess.Popen(cmd, stdout=sys.stdout, stderr=sys.stderr)\n",
        "process.wait()\n",
        "\n",
        "if process.returncode == 0:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"  ALL TRAINING + EVALUATION COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "else:\n",
        "    print(f\"\\nProcess exited with code {process.returncode}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/NanoMamba-Interspeech2026 && git pull && python -u train_all_models.py \\\n",
        "    --data_dir ./data \\\n",
        "    --checkpoint_dir ./checkpoints_freqconv \\\n",
        "    --epochs 30 --batch_size 64 --seed 42 \\\n",
        "    --models NanoMamba-Tiny-FC \\\n",
        "    --noise_types factory,white,babble \\\n",
        "    --snr_range=-15,-10,-5,0,5,10,15 \\\n",
        "    --per_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "YX4-RApk-dkB",
        "outputId": "0b01d54f-2a8d-4f8b-f170-50338a23cfb3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1468492428.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1468492428.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    cd /content/NanoMamba-Interspeech2026 && git pull && python -u train_all_models.py \\\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 핵심 모델: WS-MoE (BC-ResNet-1의 절반)\n",
        "!cd /content/NanoMamba-Interspeech2026 && git pull && python -u train_all_models.py \\\n",
        "    --data_dir ./data \\\n",
        "    --checkpoint_dir ./checkpoints_moe \\\n",
        "    --epochs 30 --batch_size 64 --seed 42 \\\n",
        "    --models NanoMamba-Tiny-MoE,NanoMamba-Tiny-WS-MoE \\\n",
        "    --noise_types factory,white,babble \\\n",
        "    --snr_range=-15,-10,-5,0,5,10,15 \\\n",
        "    --per_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7so56--DS5vz",
        "outputId": "a3525397-7e29-4b00-a51d-4b5634122883"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: cd: /content/NanoMamba-Interspeech2026: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77Fn-BjtBpeD"
      },
      "source": [
        "#@title Cell 3: 결과 확인 + 다운로드\n",
        "import json\n",
        "import glob\n",
        "\n",
        "# Find result files\n",
        "result_files = glob.glob('checkpoints_full/results/*.json')\n",
        "print(f\"Found {len(result_files)} result files:\")\n",
        "for f in sorted(result_files):\n",
        "    print(f\"  {f}\")\n",
        "\n",
        "# Show latest results\n",
        "if result_files:\n",
        "    latest = sorted(result_files)[-1]\n",
        "    with open(latest) as f:\n",
        "        results = json.load(f)\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"  Results from: {latest}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Clean accuracy table\n",
        "    if 'model_results' in results:\n",
        "        print(f\"\\n{'Model':<30} {'Params':>8} {'Val':>8} {'Test':>8}\")\n",
        "        print('-' * 58)\n",
        "        for name, data in results['model_results'].items():\n",
        "            val = data.get('best_val_acc', '-')\n",
        "            test = data.get('test_acc', '-')\n",
        "            params = data.get('params', '-')\n",
        "            val_str = f\"{val:.2f}%\" if isinstance(val, (int, float)) else str(val)\n",
        "            test_str = f\"{test:.2f}%\" if isinstance(test, (int, float)) else str(test)\n",
        "            print(f\"{name:<30} {str(params):>8} {val_str:>8} {test_str:>8}\")\n",
        "\n",
        "    # Noise robustness table\n",
        "    if 'noise_results' in results:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"  Noise Robustness Results\")\n",
        "        print(f\"{'='*70}\")\n",
        "        noise_data = results['noise_results']\n",
        "        for model_name, model_noise in noise_data.items():\n",
        "            print(f\"\\n  {model_name}:\")\n",
        "            for noise_type, snr_results in model_noise.items():\n",
        "                snr_str = \", \".join([f\"{snr}dB:{acc:.1f}%\"\n",
        "                                     for snr, acc in sorted(snr_results.items(),\n",
        "                                     key=lambda x: float(x[0]) if x[0] != 'clean' else 999)])\n",
        "                print(f\"    {noise_type}: {snr_str}\")\n",
        "\n",
        "# Zip all results for download\n",
        "!zip -r /content/smartear_results.zip checkpoints_full/\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/smartear_results.zip')\n",
        "print(\"\\nResults downloaded!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/NanoMamba-Interspeech2026/checkpoints_full/NanoMamba-Small/best.pt')\n"
      ],
      "metadata": {
        "id": "Id8otI_7Hfoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# BC-ResNet-3 Teacher 학습 (새 Colab 세션)\n",
        "# ============================================================\n",
        "\n",
        "# 1. 저장소 클론 + 데이터 준비\n",
        "!git clone https://github.com/DrJinHoChoi/NanoMamba-Interspeech2026.git\n",
        "%cd /content/NanoMamba-Interspeech2026\n",
        "\n",
        "# 2. BC-ResNet-3 학습 (~2시간)\n",
        "!python -u train_all_models.py \\\n",
        "    --data_dir /content/NanoMamba-Interspeech2026/data \\\n",
        "    --checkpoint_dir /content/NanoMamba-Interspeech2026/checkpoints_teacher \\\n",
        "    --epochs 30 --batch_size 64 --seed 42 \\\n",
        "    --models BC-ResNet-3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58rga2HCLK6X",
        "outputId": "95018cd2-1eea-420b-b23b-8f2f19f5405b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NanoMamba-Interspeech2026'...\n",
            "remote: Enumerating objects: 99, done.\u001b[K\n",
            "remote: Counting objects: 100% (99/99), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 99 (delta 40), reused 72 (delta 21), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (99/99), 1.02 MiB | 18.97 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n",
            "/content/NanoMamba-Interspeech2026\n",
            "\n",
            "================================================================================\n",
            "  SmartEar KWS - Complete Training Pipeline\n",
            "  Device: cuda\n",
            "  Data: /content/NanoMamba-Interspeech2026/data\n",
            "  Epochs: 30, Seed: 42\n",
            "  Noise types: factory,white,babble,street,pink\n",
            "  SNR range: -15,-10,-5,0,5,10,15\n",
            "  Time: 2026-02-22 02:31:10\n",
            "================================================================================\n",
            "\n",
            "  Loading Google Speech Commands V2...\n",
            "  Downloading Google Speech Commands V2 to /content/NanoMamba-Interspeech2026/data...\n",
            "100% 2.26G/2.26G [00:10<00:00, 242MB/s]\n",
            "  torchaudio download failed: unsupported operand type(s) for /: 'str' and 'str'\n",
            "  Trying manual download...\n",
            "  Manual download complete!\n",
            "  [training] 86843 samples, 12 classes\n",
            "  [validation] 10481 samples, 12 classes\n",
            "  [testing] 11505 samples, 12 classes\n",
            "\n",
            "  Train: 86843, Val: 10481, Test: 11505\n",
            "\n",
            "  Model Summary:\n",
            "  Name                   |     Params |  Size (KB)\n",
            "  --------------------------------------------------\n",
            "  NanoKWS-Tiny           |      1,354 |       5.3\n",
            "  NanoKWS-Small          |      2,144 |       8.4\n",
            "  NanoKWS-Base           |      4,428 |      17.3\n",
            "  NanoMamba-Tiny         |      4,636 |      18.1\n",
            "  NanoMamba-Small        |     12,035 |      47.0\n",
            "  NanoMamba-Base         |     40,738 |     159.1\n",
            "  NanoMamba-Tiny-FF      |      4,893 |      19.1\n",
            "  NanoMamba-Small-FF     |     12,292 |      48.0\n",
            "  NanoMamba-Tiny-WS      |      3,761 |      14.7\n",
            "  NanoMamba-Tiny-WS-FF   |      4,018 |      15.7\n",
            "  DS-CNN-S               |     23,756 |      92.8\n",
            "  MatchboxNet            |    346,444 |    1353.3\n",
            "  KWM-Small              |    389,388 |    1521.0\n",
            "  BC-ResNet-1            |      7,464 |      29.2\n",
            "  BC-ResNet-2            |     21,860 |      85.4\n",
            "  BC-ResNet-3            |     43,200 |     168.8\n",
            "  BC-ResNet-6            |    148,884 |     581.6\n",
            "  BC-ResNet-8            |    254,060 |     992.4\n",
            "  Joint-BCRes3           |     43,202 |     168.8\n",
            "  Joint-BCRes6           |    148,886 |     581.6\n",
            "  NanoMamba-Tiny-Full    |      4,636 |      18.1\n",
            "  NanoMamba-Tiny-dtOnly  |      4,636 |      18.1\n",
            "  NanoMamba-Tiny-bOnly   |      4,636 |      18.1\n",
            "  NanoMamba-Tiny-Standard |      4,636 |      18.1\n",
            "\n",
            "======================================================================\n",
            "  Training: BC-ResNet-3\n",
            "  Parameters: 43,200\n",
            "  Epochs: 30, Batch: 64, LR: 0.001\n",
            "======================================================================\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 1.8391 Acc: 59.5%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 1.7046 Acc: 61.6%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 1.6255 Acc: 62.4%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 1.5566 Acc: 63.5%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 1.4956 Acc: 64.8%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 1.4470 Acc: 66.1%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 1.4021 Acc: 67.5%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 1.3626 Acc: 68.7%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 1.3262 Acc: 69.9%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 1.2924 Acc: 71.1%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 1.2622 Acc: 72.2%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 1.2348 Acc: 73.2%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 1.2099 Acc: 74.1%\n",
            "  Epoch 1/30 | Loss: 1.1972 | Train: 74.5% | Val: 82.8% | Time: 532.1s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.8884 Acc: 86.1%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.8760 Acc: 86.5%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.8686 Acc: 86.8%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.8603 Acc: 87.2%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.8507 Acc: 87.6%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.8460 Acc: 87.7%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.8397 Acc: 88.0%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.8352 Acc: 88.2%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.8308 Acc: 88.4%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.8264 Acc: 88.6%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.8211 Acc: 88.8%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.8178 Acc: 88.9%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.8144 Acc: 89.0%\n",
            "  Epoch 2/30 | Loss: 0.8118 | Train: 89.1% | Val: 90.5% | Time: 528.1s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.7453 Acc: 91.8%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.7499 Acc: 91.6%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.7482 Acc: 91.7%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.7471 Acc: 91.8%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.7463 Acc: 91.8%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.7460 Acc: 91.8%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.7441 Acc: 91.9%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.7428 Acc: 91.9%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.7409 Acc: 92.0%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.7407 Acc: 92.0%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.7391 Acc: 92.1%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.7370 Acc: 92.1%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.7349 Acc: 92.2%\n",
            "  Epoch 3/30 | Loss: 0.7348 | Train: 92.2% | Val: 93.6% | Time: 527.7s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.7133 Acc: 93.3%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.7140 Acc: 93.1%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.7138 Acc: 93.1%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.7112 Acc: 93.2%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.7103 Acc: 93.2%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.7097 Acc: 93.2%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.7087 Acc: 93.2%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.7078 Acc: 93.3%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.7056 Acc: 93.4%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.7045 Acc: 93.4%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.7041 Acc: 93.4%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.7037 Acc: 93.4%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.7023 Acc: 93.5%\n",
            "  Epoch 4/30 | Loss: 0.7022 | Train: 93.5% | Val: 93.8% | Time: 526.5s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.6913 Acc: 94.1%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.6865 Acc: 94.1%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.6841 Acc: 94.2%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.6873 Acc: 94.0%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.6844 Acc: 94.1%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.6844 Acc: 94.1%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.6848 Acc: 94.1%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.6846 Acc: 94.1%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.6837 Acc: 94.2%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.6824 Acc: 94.2%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.6810 Acc: 94.3%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.6812 Acc: 94.3%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.6809 Acc: 94.3%\n",
            "  Epoch 5/30 | Loss: 0.6811 | Train: 94.3% | Val: 95.8% | Time: 544.0s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.6764 Acc: 94.3%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.6730 Acc: 94.4%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.6715 Acc: 94.6%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.6703 Acc: 94.7%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.6693 Acc: 94.7%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.6685 Acc: 94.8%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.6692 Acc: 94.7%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.6702 Acc: 94.7%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.6702 Acc: 94.6%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.6705 Acc: 94.6%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.6710 Acc: 94.6%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.6705 Acc: 94.6%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.6694 Acc: 94.7%\n",
            "  Epoch 6/30 | Loss: 0.6690 | Train: 94.7% | Val: 95.8% | Time: 542.3s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.6571 Acc: 95.4%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.6579 Acc: 95.3%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.6582 Acc: 95.2%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.6576 Acc: 95.1%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.6580 Acc: 95.2%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.6596 Acc: 95.1%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.6595 Acc: 95.1%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.6594 Acc: 95.1%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.6589 Acc: 95.1%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.6584 Acc: 95.1%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.6582 Acc: 95.1%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.6588 Acc: 95.1%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.6581 Acc: 95.1%\n",
            "  Epoch 7/30 | Loss: 0.6577 | Train: 95.1% | Val: 96.0% | Time: 521.4s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.6597 Acc: 95.0%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.6520 Acc: 95.4%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.6532 Acc: 95.3%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.6507 Acc: 95.4%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.6513 Acc: 95.4%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.6513 Acc: 95.3%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.6513 Acc: 95.4%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.6513 Acc: 95.4%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.6521 Acc: 95.3%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.6509 Acc: 95.4%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.6500 Acc: 95.4%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.6504 Acc: 95.4%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.6506 Acc: 95.4%\n",
            "  Epoch 8/30 | Loss: 0.6499 | Train: 95.4% | Val: 96.3% | Time: 531.6s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.6371 Acc: 96.0%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.6350 Acc: 96.0%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.6356 Acc: 96.0%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.6381 Acc: 95.9%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.6368 Acc: 96.1%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.6391 Acc: 95.9%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.6397 Acc: 95.9%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.6400 Acc: 95.8%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.6399 Acc: 95.8%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.6406 Acc: 95.8%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.6411 Acc: 95.8%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.6416 Acc: 95.7%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.6420 Acc: 95.7%\n",
            "  Epoch 9/30 | Loss: 0.6412 | Train: 95.8% | Val: 96.4% | Time: 528.6s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.6389 Acc: 95.6%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.6399 Acc: 95.7%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.6387 Acc: 95.8%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.6375 Acc: 95.8%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.6379 Acc: 95.8%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.6367 Acc: 95.9%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.6370 Acc: 95.9%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.6375 Acc: 95.8%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.6372 Acc: 95.9%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.6374 Acc: 95.9%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.6371 Acc: 95.9%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.6367 Acc: 95.9%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.6362 Acc: 95.9%\n",
            "  Epoch 10/30 | Loss: 0.6371 | Train: 95.9% | Val: 96.6% | Time: 524.7s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.6327 Acc: 96.0%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.6294 Acc: 96.2%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.6301 Acc: 96.2%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.6306 Acc: 96.2%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.6302 Acc: 96.2%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.6309 Acc: 96.2%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.6316 Acc: 96.2%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.6322 Acc: 96.1%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.6317 Acc: 96.2%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.6317 Acc: 96.1%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.6313 Acc: 96.2%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.6312 Acc: 96.2%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.6311 Acc: 96.2%\n",
            "  Epoch 11/30 | Loss: 0.6311 | Train: 96.2% | Val: 96.9% | Time: 532.2s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.6239 Acc: 96.3%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.6245 Acc: 96.3%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.6260 Acc: 96.2%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.6253 Acc: 96.3%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.6256 Acc: 96.3%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.6258 Acc: 96.3%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.6257 Acc: 96.3%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.6259 Acc: 96.3%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.6257 Acc: 96.3%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.6260 Acc: 96.3%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.6264 Acc: 96.3%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.6263 Acc: 96.3%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.6262 Acc: 96.3%\n",
            "  Epoch 12/30 | Loss: 0.6260 | Train: 96.3% | Val: 97.0% | Time: 533.5s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.6156 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.6232 Acc: 96.5%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.6242 Acc: 96.4%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.6251 Acc: 96.3%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.6245 Acc: 96.3%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.6242 Acc: 96.4%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.6227 Acc: 96.4%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.6229 Acc: 96.4%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.6226 Acc: 96.5%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.6227 Acc: 96.4%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.6226 Acc: 96.4%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.6221 Acc: 96.5%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.6216 Acc: 96.5%\n",
            "  Epoch 13/30 | Loss: 0.6216 | Train: 96.5% | Val: 96.9% | Time: 533.0s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.6155 Acc: 96.7%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.6136 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.6135 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.6150 Acc: 96.7%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.6159 Acc: 96.7%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.6163 Acc: 96.7%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.6171 Acc: 96.7%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.6176 Acc: 96.7%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.6185 Acc: 96.6%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.6182 Acc: 96.6%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.6182 Acc: 96.6%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.6186 Acc: 96.6%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.6180 Acc: 96.6%\n",
            "  Epoch 14/30 | Loss: 0.6180 | Train: 96.6% | Val: 97.1% | Time: 534.5s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.6091 Acc: 96.9%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.6116 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.6117 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.6119 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.6127 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.6111 Acc: 96.9%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.6118 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.6112 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.6120 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.6124 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.6124 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.6126 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.6124 Acc: 96.8%\n",
            "  Epoch 15/30 | Loss: 0.6124 | Train: 96.8% | Val: 97.0% | Time: 535.6s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.6062 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.6091 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.6115 Acc: 96.7%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.6114 Acc: 96.7%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.6118 Acc: 96.7%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.6118 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.6112 Acc: 96.8%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.6105 Acc: 96.9%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.6107 Acc: 96.9%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.6104 Acc: 96.9%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.6106 Acc: 96.9%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.6107 Acc: 96.9%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.6106 Acc: 96.8%\n",
            "  Epoch 16/30 | Loss: 0.6104 | Train: 96.9% | Val: 97.2% | Time: 534.7s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.6033 Acc: 97.1%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.6077 Acc: 97.0%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.6091 Acc: 96.9%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.6095 Acc: 96.9%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.6086 Acc: 97.0%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.6079 Acc: 97.0%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.6073 Acc: 97.0%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.6073 Acc: 97.0%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.6064 Acc: 97.1%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.6062 Acc: 97.1%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.6070 Acc: 97.0%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.6070 Acc: 97.1%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.6071 Acc: 97.1%\n",
            "  Epoch 17/30 | Loss: 0.6073 | Train: 97.0% | Val: 97.3% | Time: 531.7s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.6037 Acc: 97.2%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.6039 Acc: 97.2%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.6045 Acc: 97.2%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.6063 Acc: 97.1%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.6067 Acc: 97.1%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.6053 Acc: 97.1%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.6048 Acc: 97.1%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.6040 Acc: 97.2%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.6043 Acc: 97.1%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.6042 Acc: 97.2%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.6042 Acc: 97.2%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.6042 Acc: 97.2%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.6042 Acc: 97.1%\n",
            "  Epoch 18/30 | Loss: 0.6045 | Train: 97.1% | Val: 97.4% | Time: 533.2s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.5994 Acc: 97.2%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.5991 Acc: 97.3%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.5989 Acc: 97.3%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.5984 Acc: 97.3%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.5992 Acc: 97.3%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.5989 Acc: 97.3%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.6001 Acc: 97.3%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.6002 Acc: 97.3%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.5993 Acc: 97.3%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.5996 Acc: 97.3%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.6002 Acc: 97.3%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.6004 Acc: 97.3%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.6006 Acc: 97.3%\n",
            "  Epoch 19/30 | Loss: 0.6007 | Train: 97.3% | Val: 97.3% | Time: 531.7s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.5972 Acc: 97.4%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.5965 Acc: 97.5%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.5979 Acc: 97.4%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.5983 Acc: 97.4%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.5991 Acc: 97.4%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.5988 Acc: 97.4%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.5977 Acc: 97.4%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.5976 Acc: 97.5%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.5979 Acc: 97.4%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.5986 Acc: 97.4%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.5986 Acc: 97.4%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.5983 Acc: 97.4%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.5982 Acc: 97.4%\n",
            "  Epoch 20/30 | Loss: 0.5980 | Train: 97.4% | Val: 97.2% | Time: 536.0s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.5942 Acc: 97.5%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.5952 Acc: 97.5%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.5955 Acc: 97.5%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.5958 Acc: 97.5%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.5955 Acc: 97.5%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.5961 Acc: 97.5%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.5956 Acc: 97.5%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.5964 Acc: 97.4%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.5969 Acc: 97.4%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.5967 Acc: 97.4%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.5963 Acc: 97.4%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.5962 Acc: 97.4%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.5959 Acc: 97.5%\n",
            "  Epoch 21/30 | Loss: 0.5960 | Train: 97.5% | Val: 97.4% | Time: 539.0s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.5994 Acc: 97.5%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.5952 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.5919 Acc: 97.7%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.5929 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.5938 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.5938 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.5955 Acc: 97.5%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.5948 Acc: 97.5%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.5944 Acc: 97.5%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.5941 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.5937 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.5936 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.5937 Acc: 97.6%\n",
            "  Epoch 22/30 | Loss: 0.5937 | Train: 97.6% | Val: 97.4% | Time: 535.8s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.5931 Acc: 97.5%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.5903 Acc: 97.7%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.5920 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.5926 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.5920 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.5927 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.5932 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.5927 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.5937 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.5933 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.5928 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.5921 Acc: 97.6%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.5925 Acc: 97.6%\n",
            "  Epoch 23/30 | Loss: 0.5925 | Train: 97.6% | Val: 97.4% | Time: 534.6s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.5808 Acc: 98.3%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.5862 Acc: 98.0%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.5890 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.5905 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.5900 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.5905 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.5899 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.5898 Acc: 97.7%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.5894 Acc: 97.7%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.5893 Acc: 97.7%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.5899 Acc: 97.7%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.5906 Acc: 97.7%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.5905 Acc: 97.7%\n",
            "  Epoch 24/30 | Loss: 0.5906 | Train: 97.7% | Val: 97.5% | Time: 533.4s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.5844 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.5844 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.5862 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.5874 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.5878 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.5877 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.5884 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.5884 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.5887 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.5888 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.5892 Acc: 97.7%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.5889 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.5894 Acc: 97.7%\n",
            "  Epoch 25/30 | Loss: 0.5892 | Train: 97.7% | Val: 97.5% | Time: 535.8s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.5866 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.5891 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.5858 Acc: 98.0%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.5870 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.5877 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.5876 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.5873 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.5870 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.5870 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.5871 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.5879 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.5884 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.5882 Acc: 97.8%\n",
            "  Epoch 26/30 | Loss: 0.5883 | Train: 97.8% | Val: 97.4% | Time: 534.8s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.5824 Acc: 98.1%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.5834 Acc: 98.0%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.5840 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.5838 Acc: 98.0%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.5835 Acc: 98.0%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.5835 Acc: 98.0%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.5836 Acc: 98.0%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.5849 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.5853 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.5856 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.5866 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.5868 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.5868 Acc: 97.9%\n",
            "  Epoch 27/30 | Loss: 0.5867 | Train: 97.9% | Val: 97.5% | Time: 536.9s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.5901 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.5872 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.5864 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.5860 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.5872 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.5863 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.5859 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.5862 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.5868 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.5870 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.5865 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.5867 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.5863 Acc: 97.9%\n",
            "  Epoch 28/30 | Loss: 0.5863 | Train: 97.9% | Val: 97.5% | Time: 533.2s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.5889 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.5880 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.5890 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.5881 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.5883 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.5880 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.5882 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.5870 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.5864 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.5866 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.5862 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.5861 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.5860 Acc: 97.9%\n",
            "  Epoch 29/30 | Loss: 0.5863 | Train: 97.9% | Val: 97.5% | Time: 533.9s\n",
            "    [BC-ResNet-3] Batch 100/1356 Loss: 0.5869 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 200/1356 Loss: 0.5882 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 300/1356 Loss: 0.5860 Acc: 97.8%\n",
            "    [BC-ResNet-3] Batch 400/1356 Loss: 0.5851 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 500/1356 Loss: 0.5849 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 600/1356 Loss: 0.5848 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 700/1356 Loss: 0.5847 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 800/1356 Loss: 0.5849 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 900/1356 Loss: 0.5854 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 1000/1356 Loss: 0.5858 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 1100/1356 Loss: 0.5854 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 1200/1356 Loss: 0.5857 Acc: 97.9%\n",
            "    [BC-ResNet-3] Batch 1300/1356 Loss: 0.5858 Acc: 97.9%\n",
            "  Epoch 30/30 | Loss: 0.5856 | Train: 97.9% | Val: 97.4% | Time: 533.6s\n",
            "\n",
            "  Best: 97.53% @ epoch 25\n",
            "  Saved to /content/NanoMamba-Interspeech2026/checkpoints_teacher/BC-ResNet-3\n",
            "\n",
            "================================================================================\n",
            "  TEST SET EVALUATION\n",
            "================================================================================\n",
            "  BC-ResNet-3            | Test: 97.68% | Params: 43,200\n",
            "\n",
            "================================================================================\n",
            "  NOISE ROBUSTNESS EVALUATION (Audio-Domain, Unified)\n",
            "  Noise types: ['factory', 'white', 'babble', 'street', 'pink']\n",
            "  SNR levels: [-15, -10, -5, 0, 5, 10, 15, 'clean']\n",
            "================================================================================\n",
            "\n",
            "  Evaluating: BC-ResNet-3\n",
            "    factory    | Clean: 97.4% | 0dB: 78.6%\n",
            "    white      | Clean: 97.4% | 0dB: 64.3%\n",
            "    babble     | Clean: 97.4% | 0dB: 82.5%\n",
            "    street     | Clean: 97.4% | 0dB: 73.7%\n",
            "    pink       | Clean: 97.4% | 0dB: 80.2%\n",
            "\n",
            "  === Factory Noise Summary ===\n",
            "  Model                  |   Clean |    -15dB |    -10dB |     -5dB |      0dB |      5dB |     10dB |     15dB\n",
            "  -------------------------------------------------------------------------------------------\n",
            "  BC-ResNet-3            |   97.4% |   58.2% |   63.0% |   70.4% |   78.6% |   86.6% |   90.8% |   93.6%\n",
            "\n",
            "================================================================================\n",
            "  FINAL RESULTS - Paper Table\n",
            "================================================================================\n",
            "\n",
            "  Model                  |   Params |    FP32 |   INT8* |    Val |   Test | Noisy -5dB\n",
            "  ------------------------------------------------------------------------------------------\n",
            "  BC-ResNet-3            |   43,200 |  168.8K |   42.2K |  97.5% |  97.7% |      70.4%\n",
            "\n",
            "  Results saved to: /content/NanoMamba-Interspeech2026/checkpoints_teacher/results/final_results.json\n",
            "  * INT8 = estimated TensorRT INT8 size for Jetson Nano\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FF + WS 학습\n",
        "!cd /content/NanoMamba-Interspeech2026 && python -u train_all_models.py \\\n",
        "    --data_dir ./data \\\n",
        "    --checkpoint_dir ./checkpoints_variants \\\n",
        "    --epochs 30 --batch_size 64 --seed 42 \\\n",
        "    --models NanoMamba-Tiny-FF,NanoMamba-Small-FF,NanoMamba-Tiny-WS,NanoMamba-Tiny-WS-FF \\\n",
        "    --noise_types factory,white,babble \\\n",
        "    --snr_range=-15,-10,-5,0,5,10,15 \\\n",
        "    --per_class\n"
      ],
      "metadata": {
        "id": "VDu2uslIBhsZ",
        "outputId": "d18b330f-6eb9-498b-9433-0ce94b798368",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "  SmartEar KWS - Complete Training Pipeline\n",
            "  Device: cuda\n",
            "  Data: ./data\n",
            "  Epochs: 30, Seed: 42\n",
            "  Noise types: factory,white,babble\n",
            "  SNR range: -15,-10,-5,0,5,10,15\n",
            "  Time: 2026-02-22 07:37:43\n",
            "================================================================================\n",
            "\n",
            "  Loading Google Speech Commands V2...\n",
            "  [training] 86843 samples, 12 classes\n",
            "  [validation] 10481 samples, 12 classes\n",
            "  [testing] 11505 samples, 12 classes\n",
            "\n",
            "  Train: 86843, Val: 10481, Test: 11505\n",
            "\n",
            "  Model Summary:\n",
            "  Name                   |     Params |  Size (KB)\n",
            "  --------------------------------------------------\n",
            "  NanoKWS-Tiny           |      1,354 |       5.3\n",
            "  NanoKWS-Small          |      2,144 |       8.4\n",
            "  NanoKWS-Base           |      4,428 |      17.3\n",
            "  NanoMamba-Tiny         |      4,636 |      18.1\n",
            "  NanoMamba-Small        |     12,035 |      47.0\n",
            "  NanoMamba-Base         |     40,738 |     159.1\n",
            "  NanoMamba-Tiny-FF      |      4,893 |      19.1\n",
            "  NanoMamba-Small-FF     |     12,292 |      48.0\n",
            "  NanoMamba-Tiny-WS      |      3,761 |      14.7\n",
            "  NanoMamba-Tiny-WS-FF   |      4,018 |      15.7\n",
            "  DS-CNN-S               |     23,756 |      92.8\n",
            "  MatchboxNet            |    346,444 |    1353.3\n",
            "  KWM-Small              |    389,388 |    1521.0\n",
            "  BC-ResNet-1            |      7,464 |      29.2\n",
            "  BC-ResNet-2            |     21,860 |      85.4\n",
            "  BC-ResNet-3            |     43,200 |     168.8\n",
            "  BC-ResNet-6            |    148,884 |     581.6\n",
            "  BC-ResNet-8            |    254,060 |     992.4\n",
            "  Joint-BCRes3           |     43,202 |     168.8\n",
            "  Joint-BCRes6           |    148,886 |     581.6\n",
            "  NanoMamba-Tiny-Full    |      4,636 |      18.1\n",
            "  NanoMamba-Tiny-dtOnly  |      4,636 |      18.1\n",
            "  NanoMamba-Tiny-bOnly   |      4,636 |      18.1\n",
            "  NanoMamba-Tiny-Standard |      4,636 |      18.1\n",
            "\n",
            "======================================================================\n",
            "  Training: NanoMamba-Tiny-FF\n",
            "  Parameters: 4,893\n",
            "  Epochs: 30, Batch: 64, LR: 0.003\n",
            "======================================================================\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 1.7705 Acc: 59.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 1.6834 Acc: 61.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 1.6213 Acc: 62.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 1.5606 Acc: 63.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 1.5074 Acc: 64.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 1.4614 Acc: 65.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 1.4150 Acc: 67.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 1.3769 Acc: 68.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 1.3432 Acc: 69.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 1.3146 Acc: 70.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 1.2878 Acc: 71.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 1.2636 Acc: 72.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 1.2421 Acc: 73.5%\n",
            "  Epoch 1/30 | Loss: 1.2305 | Train: 74.0% | Val: 85.5% | Time: 649.6s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.9466 Acc: 85.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.9412 Acc: 85.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.9404 Acc: 85.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.9349 Acc: 85.4%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.9274 Acc: 85.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.9245 Acc: 85.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.9213 Acc: 86.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.9183 Acc: 86.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.9140 Acc: 86.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.9127 Acc: 86.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.9092 Acc: 86.4%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.9064 Acc: 86.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.9032 Acc: 86.7%\n",
            "  Epoch 2/30 | Loss: 0.9005 | Train: 86.7% | Val: 88.0% | Time: 649.2s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.8551 Acc: 88.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.8575 Acc: 88.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.8565 Acc: 88.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.8569 Acc: 88.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.8546 Acc: 88.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.8519 Acc: 88.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.8504 Acc: 88.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.8501 Acc: 88.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.8491 Acc: 88.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.8493 Acc: 88.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.8470 Acc: 88.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.8449 Acc: 88.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.8425 Acc: 88.9%\n",
            "  Epoch 3/30 | Loss: 0.8424 | Train: 88.9% | Val: 88.5% | Time: 647.9s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.8235 Acc: 89.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.8220 Acc: 89.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.8246 Acc: 89.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.8219 Acc: 89.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.8228 Acc: 89.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.8217 Acc: 89.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.8191 Acc: 89.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.8180 Acc: 89.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.8163 Acc: 89.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.8144 Acc: 89.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.8146 Acc: 89.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.8130 Acc: 89.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.8127 Acc: 89.8%\n",
            "  Epoch 4/30 | Loss: 0.8126 | Train: 89.8% | Val: 89.8% | Time: 641.8s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.8017 Acc: 90.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7998 Acc: 90.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7981 Acc: 90.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.8008 Acc: 90.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7976 Acc: 90.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7976 Acc: 90.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7977 Acc: 90.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7963 Acc: 90.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7954 Acc: 90.4%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7940 Acc: 90.4%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7927 Acc: 90.4%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7932 Acc: 90.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7930 Acc: 90.4%\n",
            "  Epoch 5/30 | Loss: 0.7928 | Train: 90.5% | Val: 91.2% | Time: 642.0s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7954 Acc: 90.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7867 Acc: 90.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7816 Acc: 90.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7812 Acc: 90.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7794 Acc: 91.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7775 Acc: 91.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7776 Acc: 91.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7790 Acc: 91.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7792 Acc: 90.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7795 Acc: 90.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7799 Acc: 90.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7794 Acc: 90.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7784 Acc: 91.0%\n",
            "  Epoch 6/30 | Loss: 0.7783 | Train: 91.0% | Val: 91.0% | Time: 647.6s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7583 Acc: 91.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7678 Acc: 91.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7723 Acc: 91.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7713 Acc: 91.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7723 Acc: 91.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7705 Acc: 91.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7715 Acc: 91.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7702 Acc: 91.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7697 Acc: 91.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7696 Acc: 91.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7687 Acc: 91.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7687 Acc: 91.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7686 Acc: 91.4%\n",
            "  Epoch 7/30 | Loss: 0.7683 | Train: 91.4% | Val: 90.0% | Time: 644.6s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7705 Acc: 91.4%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7608 Acc: 91.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7647 Acc: 91.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7637 Acc: 91.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7645 Acc: 91.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7658 Acc: 91.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7650 Acc: 91.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7652 Acc: 91.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7653 Acc: 91.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7638 Acc: 91.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7622 Acc: 91.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7627 Acc: 91.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7628 Acc: 91.6%\n",
            "  Epoch 8/30 | Loss: 0.7624 | Train: 91.6% | Val: 92.0% | Time: 633.6s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7586 Acc: 91.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7538 Acc: 92.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7502 Acc: 92.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7523 Acc: 92.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7518 Acc: 92.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7547 Acc: 91.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7547 Acc: 91.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7544 Acc: 91.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7539 Acc: 91.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7548 Acc: 91.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7543 Acc: 91.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7546 Acc: 91.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7554 Acc: 91.8%\n",
            "  Epoch 9/30 | Loss: 0.7547 | Train: 91.9% | Val: 92.0% | Time: 641.3s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7458 Acc: 92.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7518 Acc: 92.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7501 Acc: 92.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7493 Acc: 92.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7505 Acc: 91.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7498 Acc: 91.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7505 Acc: 91.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7505 Acc: 91.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7493 Acc: 92.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7498 Acc: 92.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7482 Acc: 92.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7478 Acc: 92.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7479 Acc: 92.1%\n",
            "  Epoch 10/30 | Loss: 0.7477 | Train: 92.1% | Val: 91.8% | Time: 631.1s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7435 Acc: 92.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7446 Acc: 92.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7412 Acc: 92.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7399 Acc: 92.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7399 Acc: 92.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7417 Acc: 92.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7433 Acc: 92.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7427 Acc: 92.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7427 Acc: 92.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7431 Acc: 92.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7430 Acc: 92.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7426 Acc: 92.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7429 Acc: 92.2%\n",
            "  Epoch 11/30 | Loss: 0.7428 | Train: 92.2% | Val: 92.1% | Time: 630.8s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7346 Acc: 92.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7342 Acc: 92.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7334 Acc: 92.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7321 Acc: 92.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7328 Acc: 92.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7344 Acc: 92.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7343 Acc: 92.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7355 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7370 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7373 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7378 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7383 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7379 Acc: 92.5%\n",
            "  Epoch 12/30 | Loss: 0.7372 | Train: 92.5% | Val: 92.2% | Time: 633.4s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7345 Acc: 92.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7366 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7367 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7369 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7376 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7367 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7352 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7363 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7358 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7355 Acc: 92.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7339 Acc: 92.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7332 Acc: 92.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7327 Acc: 92.6%\n",
            "  Epoch 13/30 | Loss: 0.7330 | Train: 92.6% | Val: 92.6% | Time: 621.5s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7217 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7201 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7246 Acc: 93.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7260 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7275 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7267 Acc: 93.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7271 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7289 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7289 Acc: 92.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7282 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7281 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7286 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7285 Acc: 92.9%\n",
            "  Epoch 14/30 | Loss: 0.7289 | Train: 92.9% | Val: 91.6% | Time: 612.5s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7232 Acc: 93.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7282 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7276 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7274 Acc: 93.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7272 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7249 Acc: 93.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7261 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7255 Acc: 93.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7260 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7259 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7259 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7262 Acc: 92.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7268 Acc: 92.9%\n",
            "  Epoch 15/30 | Loss: 0.7268 | Train: 92.9% | Val: 92.9% | Time: 611.9s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7193 Acc: 93.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7194 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7234 Acc: 93.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7230 Acc: 93.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7233 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7237 Acc: 93.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7242 Acc: 93.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7227 Acc: 93.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7228 Acc: 93.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7238 Acc: 93.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7240 Acc: 93.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7240 Acc: 93.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7234 Acc: 93.0%\n",
            "  Epoch 16/30 | Loss: 0.7226 | Train: 93.1% | Val: 92.8% | Time: 615.6s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7217 Acc: 92.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7181 Acc: 93.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7196 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7189 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7179 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7187 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7198 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7213 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7193 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7191 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7201 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7209 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7202 Acc: 93.2%\n",
            "  Epoch 17/30 | Loss: 0.7201 | Train: 93.2% | Val: 92.6% | Time: 613.1s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7168 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7151 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7165 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7179 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7177 Acc: 93.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7167 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7164 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7156 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7164 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7168 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7169 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7162 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7169 Acc: 93.2%\n",
            "  Epoch 18/30 | Loss: 0.7168 | Train: 93.2% | Val: 92.1% | Time: 605.7s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7153 Acc: 93.3%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7151 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7102 Acc: 93.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7102 Acc: 93.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7113 Acc: 93.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7124 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7149 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7143 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7133 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7135 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7135 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7136 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7137 Acc: 93.4%\n",
            "  Epoch 19/30 | Loss: 0.7137 | Train: 93.4% | Val: 93.1% | Time: 609.7s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7060 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7052 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7070 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7107 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7110 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7117 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7115 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7111 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7118 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7120 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7119 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7121 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7120 Acc: 93.5%\n",
            "  Epoch 20/30 | Loss: 0.7120 | Train: 93.5% | Val: 93.5% | Time: 634.2s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7057 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7081 Acc: 93.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7046 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7077 Acc: 93.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7068 Acc: 93.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7068 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7077 Acc: 93.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7100 Acc: 93.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7103 Acc: 93.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7105 Acc: 93.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7100 Acc: 93.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7098 Acc: 93.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7089 Acc: 93.6%\n",
            "  Epoch 21/30 | Loss: 0.7090 | Train: 93.6% | Val: 93.2% | Time: 613.9s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7080 Acc: 93.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7069 Acc: 93.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7033 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7049 Acc: 93.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7060 Acc: 93.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7077 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7085 Acc: 93.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7082 Acc: 93.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7085 Acc: 93.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7084 Acc: 93.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7077 Acc: 93.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7079 Acc: 93.6%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7073 Acc: 93.6%\n",
            "  Epoch 22/30 | Loss: 0.7074 | Train: 93.6% | Val: 92.5% | Time: 609.7s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7089 Acc: 93.4%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7046 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7076 Acc: 93.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7071 Acc: 93.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7060 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7055 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7055 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7052 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7060 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7060 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7048 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7040 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7045 Acc: 93.8%\n",
            "  Epoch 23/30 | Loss: 0.7044 | Train: 93.8% | Val: 93.3% | Time: 604.8s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.6891 Acc: 94.4%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.6925 Acc: 94.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.6962 Acc: 94.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.6992 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.6994 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7006 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7009 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7016 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7020 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7020 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7027 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7036 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7026 Acc: 93.8%\n",
            "  Epoch 24/30 | Loss: 0.7031 | Train: 93.8% | Val: 93.0% | Time: 616.0s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.6880 Acc: 94.5%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.6908 Acc: 94.4%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.6987 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.6978 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.6996 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.6992 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7002 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7006 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.7006 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.7007 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7014 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7016 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7021 Acc: 93.9%\n",
            "  Epoch 25/30 | Loss: 0.7015 | Train: 93.9% | Val: 93.3% | Time: 617.6s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.6974 Acc: 94.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7039 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.6969 Acc: 94.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.6973 Acc: 94.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.6984 Acc: 94.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.6983 Acc: 94.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.6980 Acc: 94.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.6989 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.6983 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.6991 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.6992 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.6998 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.6994 Acc: 94.0%\n",
            "  Epoch 26/30 | Loss: 0.6998 | Train: 94.0% | Val: 93.4% | Time: 624.3s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.6987 Acc: 93.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7020 Acc: 93.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.6996 Acc: 93.8%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.6977 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.6956 Acc: 94.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.6964 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.6974 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.6988 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.6995 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.6994 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.7005 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.7004 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.7008 Acc: 93.9%\n",
            "  Epoch 27/30 | Loss: 0.7004 | Train: 93.9% | Val: 93.3% | Time: 613.7s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.7062 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7017 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7006 Acc: 94.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.7000 Acc: 94.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7017 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.6994 Acc: 94.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.6983 Acc: 94.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.6984 Acc: 94.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.6989 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.6987 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.6987 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.6982 Acc: 94.1%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.6974 Acc: 94.1%\n",
            "  Epoch 28/30 | Loss: 0.6977 | Train: 94.1% | Val: 93.5% | Time: 615.2s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.6991 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7035 Acc: 93.7%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7014 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.6997 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.7016 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.7010 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.7012 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.7008 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.6999 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.6990 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.6985 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.6980 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.6973 Acc: 94.1%\n",
            "  Epoch 29/30 | Loss: 0.6981 | Train: 94.0% | Val: 93.3% | Time: 618.3s\n",
            "    [NanoMamba-Tiny-FF] Batch 100/1356 Loss: 0.6976 Acc: 94.2%\n",
            "    [NanoMamba-Tiny-FF] Batch 200/1356 Loss: 0.7015 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 300/1356 Loss: 0.7013 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 400/1356 Loss: 0.6996 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 500/1356 Loss: 0.6975 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 600/1356 Loss: 0.6980 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 700/1356 Loss: 0.6986 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 800/1356 Loss: 0.6988 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 900/1356 Loss: 0.6988 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1000/1356 Loss: 0.6992 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1100/1356 Loss: 0.6979 Acc: 94.0%\n",
            "    [NanoMamba-Tiny-FF] Batch 1200/1356 Loss: 0.6986 Acc: 93.9%\n",
            "    [NanoMamba-Tiny-FF] Batch 1300/1356 Loss: 0.6989 Acc: 93.9%\n",
            "  Epoch 30/30 | Loss: 0.6985 | Train: 94.0% | Val: 93.3% | Time: 598.7s\n",
            "\n",
            "  Best: 93.50% @ epoch 28\n",
            "  Saved to checkpoints_variants/NanoMamba-Tiny-FF\n",
            "\n",
            "======================================================================\n",
            "  Training: NanoMamba-Small-FF\n",
            "  Parameters: 12,292\n",
            "  Epochs: 30, Batch: 64, LR: 0.003\n",
            "======================================================================\n",
            "    [NanoMamba-Small-FF] Batch 100/1356 Loss: 1.7498 Acc: 60.3%\n",
            "    [NanoMamba-Small-FF] Batch 200/1356 Loss: 1.6650 Acc: 62.2%\n",
            "    [NanoMamba-Small-FF] Batch 300/1356 Loss: 1.5925 Acc: 63.5%\n",
            "    [NanoMamba-Small-FF] Batch 400/1356 Loss: 1.5189 Acc: 64.9%\n",
            "    [NanoMamba-Small-FF] Batch 500/1356 Loss: 1.4460 Acc: 66.9%\n",
            "    [NanoMamba-Small-FF] Batch 600/1356 Loss: 1.3863 Acc: 68.8%\n",
            "    [NanoMamba-Small-FF] Batch 700/1356 Loss: 1.3371 Acc: 70.5%\n",
            "    [NanoMamba-Small-FF] Batch 800/1356 Loss: 1.2918 Acc: 72.2%\n",
            "    [NanoMamba-Small-FF] Batch 900/1356 Loss: 1.2573 Acc: 73.4%\n",
            "    [NanoMamba-Small-FF] Batch 1000/1356 Loss: 1.2238 Acc: 74.6%\n",
            "    [NanoMamba-Small-FF] Batch 1100/1356 Loss: 1.1952 Acc: 75.7%\n",
            "    [NanoMamba-Small-FF] Batch 1200/1356 Loss: 1.1700 Acc: 76.6%\n",
            "    [NanoMamba-Small-FF] Batch 1300/1356 Loss: 1.1483 Acc: 77.4%\n",
            "  Epoch 1/30 | Loss: 1.1371 | Train: 77.8% | Val: 88.6% | Time: 671.9s\n",
            "    [NanoMamba-Small-FF] Batch 100/1356 Loss: 0.8554 Acc: 88.2%\n",
            "    [NanoMamba-Small-FF] Batch 200/1356 Loss: 0.8626 Acc: 87.8%\n",
            "    [NanoMamba-Small-FF] Batch 300/1356 Loss: 0.8602 Acc: 87.9%\n",
            "    [NanoMamba-Small-FF] Batch 400/1356 Loss: 0.8548 Acc: 88.0%\n",
            "    [NanoMamba-Small-FF] Batch 500/1356 Loss: 0.8497 Acc: 88.2%\n",
            "    [NanoMamba-Small-FF] Batch 600/1356 Loss: 0.8458 Acc: 88.3%\n",
            "    [NanoMamba-Small-FF] Batch 700/1356 Loss: 0.8423 Acc: 88.4%\n",
            "    [NanoMamba-Small-FF] Batch 800/1356 Loss: 0.8385 Acc: 88.6%\n",
            "    [NanoMamba-Small-FF] Batch 900/1356 Loss: 0.8357 Acc: 88.7%\n",
            "    [NanoMamba-Small-FF] Batch 1000/1356 Loss: 0.8338 Acc: 88.8%\n",
            "    [NanoMamba-Small-FF] Batch 1100/1356 Loss: 0.8316 Acc: 88.8%\n",
            "    [NanoMamba-Small-FF] Batch 1200/1356 Loss: 0.8304 Acc: 88.9%\n",
            "    [NanoMamba-Small-FF] Batch 1300/1356 Loss: 0.8292 Acc: 88.9%\n",
            "  Epoch 2/30 | Loss: 0.8286 | Train: 89.0% | Val: 90.3% | Time: 672.1s\n",
            "    [NanoMamba-Small-FF] Batch 100/1356 Loss: 0.7918 Acc: 89.7%\n",
            "    [NanoMamba-Small-FF] Batch 200/1356 Loss: 0.7960 Acc: 89.7%\n",
            "    [NanoMamba-Small-FF] Batch 300/1356 Loss: 0.7934 Acc: 89.9%\n",
            "    [NanoMamba-Small-FF] Batch 400/1356 Loss: 0.7917 Acc: 90.0%\n",
            "    [NanoMamba-Small-FF] Batch 500/1356 Loss: 0.7887 Acc: 90.2%\n",
            "    [NanoMamba-Small-FF] Batch 600/1356 Loss: 0.7884 Acc: 90.2%\n",
            "    [NanoMamba-Small-FF] Batch 700/1356 Loss: 0.7878 Acc: 90.3%\n",
            "    [NanoMamba-Small-FF] Batch 800/1356 Loss: 0.7853 Acc: 90.5%\n",
            "    [NanoMamba-Small-FF] Batch 900/1356 Loss: 0.7842 Acc: 90.5%\n",
            "    [NanoMamba-Small-FF] Batch 1000/1356 Loss: 0.7830 Acc: 90.6%\n",
            "    [NanoMamba-Small-FF] Batch 1100/1356 Loss: 0.7817 Acc: 90.6%\n",
            "    [NanoMamba-Small-FF] Batch 1200/1356 Loss: 0.7802 Acc: 90.7%\n",
            "    [NanoMamba-Small-FF] Batch 1300/1356 Loss: 0.7792 Acc: 90.7%\n",
            "  Epoch 3/30 | Loss: 0.7784 | Train: 90.7% | Val: 91.1% | Time: 653.6s\n",
            "    [NanoMamba-Small-FF] Batch 100/1356 Loss: 0.7703 Acc: 90.9%\n",
            "    [NanoMamba-Small-FF] Batch 200/1356 Loss: 0.7688 Acc: 91.1%\n",
            "    [NanoMamba-Small-FF] Batch 300/1356 Loss: 0.7622 Acc: 91.3%\n",
            "    [NanoMamba-Small-FF] Batch 400/1356 Loss: 0.7622 Acc: 91.2%\n",
            "    [NanoMamba-Small-FF] Batch 500/1356 Loss: 0.7607 Acc: 91.3%\n",
            "    [NanoMamba-Small-FF] Batch 600/1356 Loss: 0.7596 Acc: 91.4%\n",
            "    [NanoMamba-Small-FF] Batch 700/1356 Loss: 0.7597 Acc: 91.4%\n",
            "    [NanoMamba-Small-FF] Batch 800/1356 Loss: 0.7581 Acc: 91.5%\n",
            "    [NanoMamba-Small-FF] Batch 900/1356 Loss: 0.7566 Acc: 91.6%\n",
            "    [NanoMamba-Small-FF] Batch 1000/1356 Loss: 0.7571 Acc: 91.6%\n",
            "    [NanoMamba-Small-FF] Batch 1100/1356 Loss: 0.7553 Acc: 91.6%\n",
            "    [NanoMamba-Small-FF] Batch 1200/1356 Loss: 0.7550 Acc: 91.6%\n",
            "    [NanoMamba-Small-FF] Batch 1300/1356 Loss: 0.7545 Acc: 91.7%\n",
            "  Epoch 4/30 | Loss: 0.7549 | Train: 91.6% | Val: 91.1% | Time: 645.5s\n",
            "    [NanoMamba-Small-FF] Batch 100/1356 Loss: 0.7467 Acc: 92.0%\n",
            "    [NanoMamba-Small-FF] Batch 200/1356 Loss: 0.7413 Acc: 92.0%\n",
            "    [NanoMamba-Small-FF] Batch 300/1356 Loss: 0.7415 Acc: 92.0%\n",
            "    [NanoMamba-Small-FF] Batch 400/1356 Loss: 0.7401 Acc: 92.0%\n",
            "    [NanoMamba-Small-FF] Batch 500/1356 Loss: 0.7418 Acc: 91.9%\n",
            "    [NanoMamba-Small-FF] Batch 600/1356 Loss: 0.7396 Acc: 92.1%\n",
            "    [NanoMamba-Small-FF] Batch 700/1356 Loss: 0.7401 Acc: 92.1%\n",
            "    [NanoMamba-Small-FF] Batch 800/1356 Loss: 0.7384 Acc: 92.2%\n",
            "    [NanoMamba-Small-FF] Batch 900/1356 Loss: 0.7379 Acc: 92.2%\n",
            "    [NanoMamba-Small-FF] Batch 1000/1356 Loss: 0.7372 Acc: 92.2%\n",
            "    [NanoMamba-Small-FF] Batch 1100/1356 Loss: 0.7362 Acc: 92.3%\n",
            "    [NanoMamba-Small-FF] Batch 1200/1356 Loss: 0.7354 Acc: 92.3%\n"
          ]
        }
      ]
    }
  ]
}