{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "machine_shape": "hm",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DrJinHoChoi/NanoMamba-Interspeech2026/blob/main/colab/SmartEar_KWS_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHbKJ5MvAPs5"
   },
   "source": [
    "# NanoMamba - Interspeech 2026 Full Training (GPU)\n",
    "\n",
    "**NanoMamba: Noise-Robust KWS with SA-SSM**\n",
    "\n",
    "| Cell | 내용 | 예상 시간 |\n",
    "|:----:|------|:---------:|\n",
    "| 1 | 환경설정 + GSC V2 다운로드 | ~5분 |\n",
    "| 2 | **전체 학습 + 평가 한번에** | ~8-12시간 |\n",
    "| 3 | 결과 다운로드 | 즉시 |\n",
    "\n",
    "⚠️ **런타임 → 런타임 유형 변경 → GPU (T4)** 선택 필수!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r8KwW_NeAPs6"
   },
   "source": [
    "#@title Cell 1: 환경 설정 + 데이터 다운로드\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    raise RuntimeError(\"GPU not available! Change runtime type to GPU.\")\n",
    "\n",
    "# Clone repo\n",
    "!git clone https://github.com/DrJinHoChoi/NanoMamba-Interspeech2026.git\n",
    "%cd NanoMamba-Interspeech2026\n",
    "\n",
    "# Download Google Speech Commands V2\n",
    "import os\n",
    "DATA_DIR = './data'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(os.path.join(DATA_DIR, 'speech_commands_v0.02')):\n",
    "    print(\"\\n Downloading Google Speech Commands V2...\")\n",
    "    !wget -q http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz -O /tmp/gsc_v2.tar.gz\n",
    "    !mkdir -p {DATA_DIR}/speech_commands_v0.02\n",
    "    !tar -xzf /tmp/gsc_v2.tar.gz -C {DATA_DIR}/speech_commands_v0.02\n",
    "    !rm /tmp/gsc_v2.tar.gz\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"Data already exists.\")\n",
    "\n",
    "# Verify\n",
    "classes = [d for d in os.listdir(f'{DATA_DIR}/speech_commands_v0.02')\n",
    "           if os.path.isdir(f'{DATA_DIR}/speech_commands_v0.02/{d}') and not d.startswith('_')]\n",
    "print(f\"\\nFound {len(classes)} keyword classes\")\n",
    "print(\"Ready to train!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jR8sbqS1APs6",
    "outputId": "f8815480-ae5e-44ea-ad6c-5d6956686bcf"
   },
   "source": [
    "#@title Cell 2: 전체 학습 + 노이즈 평가 (한번에 실행)\n",
    "#@markdown ### 학습 모델 (9종)\n",
    "#@markdown - NanoMamba-Tiny (4,634), Small (12,032)\n",
    "#@markdown - BC-ResNet-1 (7,464), BC-ResNet-3 (43,200), DS-CNN-S (23,756)\n",
    "#@markdown - SA-SSM Ablation: Full, dt_only, b_only, Standard\n",
    "#@markdown ### 노이즈 평가\n",
    "#@markdown - 3 noise types (factory, white, babble) x 7 SNR (-15~+15dB)\n",
    "\n",
    "import subprocess, sys\n",
    "\n",
    "ALL_MODELS = \",\".join([\n",
    "    # Proposed (Tiny 완료, Small부터)\n",
    "    \"NanoMamba-Small\",\n",
    "    # Baselines\n",
    "    \"BC-ResNet-1\", \"BC-ResNet-3\", \"DS-CNN-S\",\n",
    "    # SA-SSM Ablation\n",
    "    \"NanoMamba-Tiny-Full\", \"NanoMamba-Tiny-dtOnly\",\n",
    "    \"NanoMamba-Tiny-bOnly\", \"NanoMamba-Tiny-Standard\",\n",
    "])\n",
    "\n",
    "cmd = [\n",
    "    sys.executable, \"-u\", \"train_all_models.py\",\n",
    "    \"--data_dir\", \"./data\",\n",
    "    \"--checkpoint_dir\", \"./checkpoints_full\",\n",
    "    \"--epochs\", \"30\",\n",
    "    \"--batch_size\", \"64\",\n",
    "    \"--seed\", \"42\",\n",
    "    \"--models\", ALL_MODELS,\n",
    "    \"--noise_types\", \"factory,white,babble\",\n",
    "    \"--snr_range=-15,-10,-5,0,5,10,15\", # Changed: Use = to avoid argparse confusion with negative numbers\n",
    "    \"--per_class\",\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(cmd)}\\n\")\n",
    "\n",
    "# Capture and stream output to see errors\n",
    "process = subprocess.Popen(\n",
    "    cmd,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True,\n",
    "    bufsize=1\n",
    ")\n",
    "\n",
    "for line in process.stdout:\n",
    "    print(line, end='')\n",
    "\n",
    "process.wait()\n",
    "\n",
    "if process.returncode == 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  ALL TRAINING + EVALUATION COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(f\"\\nProcess exited with code {process.returncode}\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running: /usr/bin/python3 -u train_all_models.py --data_dir ./data --checkpoint_dir ./checkpoints_full --epochs 30 --batch_size 64 --seed 42 --models NanoMamba-Small,BC-ResNet-1,BC-ResNet-3,DS-CNN-S,NanoMamba-Tiny-Full,NanoMamba-Tiny-dtOnly,NanoMamba-Tiny-bOnly,NanoMamba-Tiny-Standard --noise_types factory,white,babble --snr_range=-15,-10,-5,0,5,10,15 --per_class\n",
      "\n",
      "\n",
      "================================================================================\n",
      "  SmartEar KWS - Complete Training Pipeline\n",
      "  Device: cuda\n",
      "  Data: ./data\n",
      "  Epochs: 30, Seed: 42\n",
      "  Noise types: factory,white,babble\n",
      "  SNR range: -15,-10,-5,0,5,10,15\n",
      "  Time: 2026-02-21 02:33:41\n",
      "================================================================================\n",
      "\n",
      "  Loading Google Speech Commands V2...\n",
      "  [training] 86843 samples, 12 classes\n",
      "  [validation] 10481 samples, 12 classes\n",
      "  [testing] 11505 samples, 12 classes\n",
      "\n",
      "  Train: 86843, Val: 10481, Test: 11505\n",
      "\n",
      "  Model Summary:\n",
      "  Name                   |     Params |  Size (KB)\n",
      "  --------------------------------------------------\n",
      "  NanoKWS-Tiny           |      1,354 |       5.3\n",
      "  NanoKWS-Small          |      2,144 |       8.4\n",
      "  NanoKWS-Base           |      4,428 |      17.3\n",
      "  NanoMamba-Tiny         |      4,634 |      18.1\n",
      "  NanoMamba-Small        |     12,032 |      47.0\n",
      "  NanoMamba-Base         |     40,734 |     159.1\n",
      "  DS-CNN-S               |     23,756 |      92.8\n",
      "  MatchboxNet            |    346,444 |    1353.3\n",
      "  KWM-Small              |    389,388 |    1521.0\n",
      "  BC-ResNet-1            |      7,464 |      29.2\n",
      "  BC-ResNet-2            |     21,860 |      85.4\n",
      "  BC-ResNet-3            |     43,200 |     168.8\n",
      "  BC-ResNet-6            |    148,884 |     581.6\n",
      "  BC-ResNet-8            |    254,060 |     992.4\n",
      "  Joint-BCRes3           |     43,202 |     168.8\n",
      "  Joint-BCRes6           |    148,886 |     581.6\n",
      "  NanoMamba-Tiny-Full    |      4,634 |      18.1\n",
      "  NanoMamba-Tiny-dtOnly  |      4,634 |      18.1\n",
      "  NanoMamba-Tiny-bOnly   |      4,634 |      18.1\n",
      "  NanoMamba-Tiny-Standard |      4,634 |      18.1\n",
      "\n",
      "======================================================================\n",
      "  Training: NanoMamba-Small\n",
      "  Parameters: 12,032\n",
      "  Epochs: 30, Batch: 64, LR: 0.003\n",
      "======================================================================\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 1.7202 Acc: 62.3%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 1.6484 Acc: 62.3%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 1.5628 Acc: 63.7%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 1.4748 Acc: 66.0%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 1.4009 Acc: 68.2%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 1.3381 Acc: 70.5%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 1.2883 Acc: 72.2%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 1.2453 Acc: 73.8%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 1.2088 Acc: 75.1%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 1.1788 Acc: 76.2%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 1.1541 Acc: 77.1%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 1.1313 Acc: 77.9%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 1.1107 Acc: 78.7%\n",
      "  Epoch 1/30 | Loss: 1.1002 | Train: 79.0% | Val: 86.4% | Time: 675.8s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.8542 Acc: 88.0%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.8504 Acc: 88.1%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.8474 Acc: 88.1%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.8433 Acc: 88.3%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.8416 Acc: 88.4%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.8373 Acc: 88.5%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.8330 Acc: 88.7%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.8307 Acc: 88.7%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.8268 Acc: 88.8%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.8245 Acc: 88.9%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.8227 Acc: 89.0%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.8208 Acc: 89.1%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.8195 Acc: 89.1%\n",
      "  Epoch 2/30 | Loss: 0.8184 | Train: 89.1% | Val: 90.4% | Time: 673.9s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.7851 Acc: 90.6%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.7808 Acc: 90.7%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.7838 Acc: 90.5%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.7827 Acc: 90.5%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.7808 Acc: 90.6%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.7797 Acc: 90.6%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.7796 Acc: 90.6%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.7764 Acc: 90.7%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.7765 Acc: 90.7%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.7746 Acc: 90.7%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.7716 Acc: 90.9%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.7700 Acc: 91.0%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.7687 Acc: 91.0%\n",
      "  Epoch 3/30 | Loss: 0.7687 | Train: 91.0% | Val: 91.0% | Time: 675.1s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.7645 Acc: 91.2%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.7603 Acc: 91.4%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.7577 Acc: 91.5%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.7544 Acc: 91.5%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.7544 Acc: 91.4%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.7509 Acc: 91.5%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.7489 Acc: 91.6%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.7478 Acc: 91.7%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.7472 Acc: 91.7%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.7460 Acc: 91.7%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.7454 Acc: 91.8%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.7448 Acc: 91.8%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.7463 Acc: 91.8%\n",
      "  Epoch 4/30 | Loss: 0.7460 | Train: 91.8% | Val: 92.3% | Time: 675.5s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.7310 Acc: 92.4%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.7291 Acc: 92.6%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.7313 Acc: 92.4%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.7313 Acc: 92.5%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.7302 Acc: 92.5%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.7300 Acc: 92.5%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.7303 Acc: 92.5%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.7315 Acc: 92.4%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.7321 Acc: 92.4%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.7323 Acc: 92.4%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.7310 Acc: 92.4%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.7310 Acc: 92.4%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.7311 Acc: 92.4%\n",
      "  Epoch 5/30 | Loss: 0.7306 | Train: 92.5% | Val: 92.8% | Time: 676.6s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6986 Acc: 93.8%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.7097 Acc: 93.4%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.7148 Acc: 93.1%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.7181 Acc: 93.0%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.7199 Acc: 92.9%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.7189 Acc: 92.9%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.7201 Acc: 92.8%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.7190 Acc: 92.9%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.7188 Acc: 92.9%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.7185 Acc: 93.0%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.7182 Acc: 93.0%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.7167 Acc: 93.0%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.7170 Acc: 93.0%\n",
      "  Epoch 6/30 | Loss: 0.7178 | Train: 93.0% | Val: 93.4% | Time: 673.2s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.7095 Acc: 93.3%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.7184 Acc: 93.0%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.7138 Acc: 93.1%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.7146 Acc: 93.1%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.7125 Acc: 93.2%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.7117 Acc: 93.2%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.7104 Acc: 93.3%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.7109 Acc: 93.3%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.7103 Acc: 93.3%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.7112 Acc: 93.3%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.7118 Acc: 93.3%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.7108 Acc: 93.3%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.7103 Acc: 93.3%\n",
      "  Epoch 7/30 | Loss: 0.7099 | Train: 93.3% | Val: 93.0% | Time: 674.4s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6991 Acc: 93.8%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.7030 Acc: 93.5%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.7009 Acc: 93.6%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.7035 Acc: 93.6%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.7037 Acc: 93.5%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.7033 Acc: 93.5%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.7011 Acc: 93.6%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.7010 Acc: 93.6%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.7018 Acc: 93.6%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.7014 Acc: 93.6%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.7021 Acc: 93.6%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.7026 Acc: 93.5%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.7026 Acc: 93.5%\n",
      "  Epoch 8/30 | Loss: 0.7019 | Train: 93.5% | Val: 94.1% | Time: 672.5s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6988 Acc: 94.0%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6929 Acc: 94.1%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6959 Acc: 93.8%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6964 Acc: 93.7%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6949 Acc: 93.8%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6947 Acc: 93.8%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6952 Acc: 93.8%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6955 Acc: 93.8%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6961 Acc: 93.8%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6948 Acc: 93.8%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6952 Acc: 93.8%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6959 Acc: 93.8%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6966 Acc: 93.8%\n",
      "  Epoch 9/30 | Loss: 0.6961 | Train: 93.8% | Val: 93.3% | Time: 674.3s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6928 Acc: 93.8%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6867 Acc: 94.1%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6877 Acc: 94.1%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6884 Acc: 94.0%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6906 Acc: 93.9%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6893 Acc: 94.0%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6893 Acc: 94.0%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6906 Acc: 93.9%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6902 Acc: 94.0%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6900 Acc: 94.0%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6893 Acc: 94.1%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6890 Acc: 94.1%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6884 Acc: 94.1%\n",
      "  Epoch 10/30 | Loss: 0.6881 | Train: 94.1% | Val: 94.0% | Time: 674.7s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6863 Acc: 94.0%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6841 Acc: 94.3%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6794 Acc: 94.5%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6837 Acc: 94.3%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6832 Acc: 94.3%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6828 Acc: 94.3%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6820 Acc: 94.3%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6823 Acc: 94.3%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6835 Acc: 94.2%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6836 Acc: 94.2%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6835 Acc: 94.3%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6830 Acc: 94.3%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6833 Acc: 94.3%\n",
      "  Epoch 11/30 | Loss: 0.6831 | Train: 94.3% | Val: 94.4% | Time: 674.3s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6727 Acc: 94.7%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6762 Acc: 94.5%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6763 Acc: 94.6%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6773 Acc: 94.6%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6765 Acc: 94.6%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6778 Acc: 94.6%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6770 Acc: 94.6%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6770 Acc: 94.6%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6759 Acc: 94.6%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6779 Acc: 94.5%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6778 Acc: 94.5%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6771 Acc: 94.5%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6771 Acc: 94.5%\n",
      "  Epoch 12/30 | Loss: 0.6771 | Train: 94.5% | Val: 94.1% | Time: 673.1s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6681 Acc: 94.9%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6719 Acc: 94.8%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6730 Acc: 94.8%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6729 Acc: 94.7%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6715 Acc: 94.8%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6714 Acc: 94.7%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6719 Acc: 94.7%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6726 Acc: 94.6%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6727 Acc: 94.7%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6727 Acc: 94.7%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6733 Acc: 94.7%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6732 Acc: 94.7%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6737 Acc: 94.7%\n",
      "  Epoch 13/30 | Loss: 0.6739 | Train: 94.7% | Val: 94.4% | Time: 673.0s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6558 Acc: 95.4%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6612 Acc: 95.3%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6642 Acc: 95.1%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6670 Acc: 95.0%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6662 Acc: 95.0%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6674 Acc: 94.9%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6675 Acc: 94.9%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6670 Acc: 94.9%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6668 Acc: 94.9%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6669 Acc: 94.9%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6664 Acc: 95.0%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6669 Acc: 95.0%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6677 Acc: 94.9%\n",
      "  Epoch 14/30 | Loss: 0.6684 | Train: 94.9% | Val: 94.3% | Time: 674.9s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6588 Acc: 95.4%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6633 Acc: 95.3%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6634 Acc: 95.2%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6609 Acc: 95.3%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6596 Acc: 95.3%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6610 Acc: 95.2%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6640 Acc: 95.0%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6628 Acc: 95.1%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6642 Acc: 95.0%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6638 Acc: 95.0%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6646 Acc: 95.0%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6642 Acc: 95.0%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6640 Acc: 95.0%\n",
      "  Epoch 15/30 | Loss: 0.6644 | Train: 95.0% | Val: 94.5% | Time: 673.3s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6599 Acc: 95.1%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6578 Acc: 95.2%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6592 Acc: 95.1%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6617 Acc: 95.1%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6611 Acc: 95.1%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6605 Acc: 95.2%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6607 Acc: 95.2%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6598 Acc: 95.2%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6594 Acc: 95.2%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6595 Acc: 95.2%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6599 Acc: 95.2%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6597 Acc: 95.2%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6594 Acc: 95.2%\n",
      "  Epoch 16/30 | Loss: 0.6597 | Train: 95.2% | Val: 94.6% | Time: 672.1s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6416 Acc: 95.8%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6506 Acc: 95.4%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6555 Acc: 95.3%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6564 Acc: 95.3%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6557 Acc: 95.3%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6564 Acc: 95.3%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6562 Acc: 95.3%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6572 Acc: 95.2%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6570 Acc: 95.3%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6571 Acc: 95.3%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6575 Acc: 95.2%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6575 Acc: 95.2%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6584 Acc: 95.2%\n",
      "  Epoch 17/30 | Loss: 0.6582 | Train: 95.2% | Val: 94.9% | Time: 673.4s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6602 Acc: 95.3%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6506 Acc: 95.6%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6521 Acc: 95.5%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6540 Acc: 95.4%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6544 Acc: 95.4%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6537 Acc: 95.4%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6529 Acc: 95.5%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6544 Acc: 95.5%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6544 Acc: 95.4%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6541 Acc: 95.4%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6544 Acc: 95.4%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6539 Acc: 95.5%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6543 Acc: 95.4%\n",
      "  Epoch 18/30 | Loss: 0.6539 | Train: 95.4% | Val: 94.8% | Time: 673.0s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6515 Acc: 95.3%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6518 Acc: 95.3%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6508 Acc: 95.4%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6504 Acc: 95.5%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6517 Acc: 95.5%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6516 Acc: 95.5%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6519 Acc: 95.5%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6524 Acc: 95.4%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6523 Acc: 95.5%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6520 Acc: 95.5%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6513 Acc: 95.5%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6510 Acc: 95.5%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6512 Acc: 95.5%\n",
      "  Epoch 19/30 | Loss: 0.6512 | Train: 95.5% | Val: 94.7% | Time: 674.2s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6489 Acc: 95.4%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6516 Acc: 95.5%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6480 Acc: 95.6%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6475 Acc: 95.6%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6457 Acc: 95.7%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6470 Acc: 95.6%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6466 Acc: 95.6%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6463 Acc: 95.7%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6457 Acc: 95.7%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6462 Acc: 95.7%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6461 Acc: 95.7%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6465 Acc: 95.7%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6468 Acc: 95.7%\n",
      "  Epoch 20/30 | Loss: 0.6469 | Train: 95.7% | Val: 94.8% | Time: 673.4s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6495 Acc: 95.6%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6430 Acc: 95.9%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6454 Acc: 95.7%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6471 Acc: 95.7%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6448 Acc: 95.8%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6448 Acc: 95.8%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6458 Acc: 95.8%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6459 Acc: 95.8%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6457 Acc: 95.8%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6460 Acc: 95.7%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6457 Acc: 95.8%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6453 Acc: 95.8%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6453 Acc: 95.8%\n",
      "  Epoch 21/30 | Loss: 0.6450 | Train: 95.8% | Val: 94.9% | Time: 673.5s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6488 Acc: 95.8%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6445 Acc: 95.9%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6432 Acc: 95.9%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6417 Acc: 96.0%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6419 Acc: 95.9%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6424 Acc: 95.9%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6430 Acc: 95.9%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6438 Acc: 95.8%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6436 Acc: 95.9%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6437 Acc: 95.8%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6433 Acc: 95.9%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6424 Acc: 95.9%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6427 Acc: 95.9%\n",
      "  Epoch 22/30 | Loss: 0.6428 | Train: 95.9% | Val: 95.1% | Time: 673.6s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6373 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6344 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6385 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6404 Acc: 96.0%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6403 Acc: 96.0%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6395 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6391 Acc: 96.0%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6408 Acc: 96.0%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6399 Acc: 96.0%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6415 Acc: 95.9%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6415 Acc: 95.9%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6421 Acc: 95.9%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6413 Acc: 95.9%\n",
      "  Epoch 23/30 | Loss: 0.6415 | Train: 95.9% | Val: 94.9% | Time: 673.5s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6308 Acc: 96.5%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6348 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6323 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6334 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6343 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6358 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6371 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6369 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6363 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6371 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6371 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6377 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6377 Acc: 96.1%\n",
      "  Epoch 24/30 | Loss: 0.6377 | Train: 96.1% | Val: 95.0% | Time: 674.1s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6353 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6356 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6364 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6370 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6379 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6378 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6374 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6373 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6365 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6364 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6367 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6370 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6371 Acc: 96.1%\n",
      "  Epoch 25/30 | Loss: 0.6369 | Train: 96.1% | Val: 95.2% | Time: 674.7s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6413 Acc: 95.9%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6404 Acc: 95.9%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6381 Acc: 96.0%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6385 Acc: 96.0%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6380 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6379 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6381 Acc: 96.0%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6385 Acc: 96.0%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6388 Acc: 96.0%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6377 Acc: 96.0%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6372 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6366 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6361 Acc: 96.1%\n",
      "  Epoch 26/30 | Loss: 0.6362 | Train: 96.1% | Val: 95.0% | Time: 673.7s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6367 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6369 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6393 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6364 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6357 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6345 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6346 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6345 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6348 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6345 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6345 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6338 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6337 Acc: 96.3%\n",
      "  Epoch 27/30 | Loss: 0.6335 | Train: 96.3% | Val: 95.2% | Time: 672.6s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6281 Acc: 96.6%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6295 Acc: 96.5%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6299 Acc: 96.5%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6309 Acc: 96.4%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6323 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6320 Acc: 96.4%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6323 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6318 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6321 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6327 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6325 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6326 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6330 Acc: 96.3%\n",
      "  Epoch 28/30 | Loss: 0.6326 | Train: 96.3% | Val: 95.2% | Time: 674.3s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6305 Acc: 96.4%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6344 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6327 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6318 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6308 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6297 Acc: 96.4%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6309 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6306 Acc: 96.4%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6315 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6312 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6316 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6315 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6324 Acc: 96.3%\n",
      "  Epoch 29/30 | Loss: 0.6323 | Train: 96.3% | Val: 95.1% | Time: 672.7s\n",
      "    [NanoMamba-Small] Batch 100/1356 Loss: 0.6284 Acc: 96.4%\n",
      "    [NanoMamba-Small] Batch 200/1356 Loss: 0.6299 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 300/1356 Loss: 0.6346 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 400/1356 Loss: 0.6342 Acc: 96.1%\n",
      "    [NanoMamba-Small] Batch 500/1356 Loss: 0.6340 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 600/1356 Loss: 0.6345 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 700/1356 Loss: 0.6342 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 800/1356 Loss: 0.6331 Acc: 96.2%\n",
      "    [NanoMamba-Small] Batch 900/1356 Loss: 0.6328 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 1000/1356 Loss: 0.6328 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 1100/1356 Loss: 0.6318 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 1200/1356 Loss: 0.6318 Acc: 96.3%\n",
      "    [NanoMamba-Small] Batch 1300/1356 Loss: 0.6314 Acc: 96.3%\n",
      "  Epoch 30/30 | Loss: 0.6315 | Train: 96.3% | Val: 95.2% | Time: 672.7s\n",
      "\n",
      "  Best: 95.24% @ epoch 28\n",
      "  Saved to checkpoints_full/NanoMamba-Small\n",
      "\n",
      "======================================================================\n",
      "  Training: DS-CNN-S\n",
      "  Parameters: 23,756\n",
      "  Epochs: 30, Batch: 64, LR: 0.001\n",
      "======================================================================\n",
      "    [DS-CNN-S] Batch 100/1356 Loss: 1.8435 Acc: 57.6%\n",
      "    [DS-CNN-S] Batch 200/1356 Loss: 1.7307 Acc: 59.9%\n",
      "    [DS-CNN-S] Batch 300/1356 Loss: 1.6634 Acc: 61.4%\n",
      "    [DS-CNN-S] Batch 400/1356 Loss: 1.6157 Acc: 62.0%\n",
      "    [DS-CNN-S] Batch 500/1356 Loss: 1.5656 Acc: 63.0%\n",
      "    [DS-CNN-S] Batch 600/1356 Loss: 1.5298 Acc: 63.6%\n",
      "    [DS-CNN-S] Batch 700/1356 Loss: 1.4995 Acc: 64.2%\n",
      "    [DS-CNN-S] Batch 800/1356 Loss: 1.4665 Acc: 65.1%\n",
      "    [DS-CNN-S] Batch 900/1356 Loss: 1.4353 Acc: 66.0%\n",
      "    [DS-CNN-S] Batch 1000/1356 Loss: 1.4051 Acc: 67.0%\n",
      "    [DS-CNN-S] Batch 1100/1356 Loss: 1.3757 Acc: 68.0%\n",
      "    [DS-CNN-S] Batch 1200/1356 Loss: 1.3466 Acc: 69.1%\n",
      "    [DS-CNN-S] Batch 1300/1356 Loss: 1.3195 Acc: 70.2%\n",
      "  Epoch 1/30 | Loss: 1.3050 | Train: 70.8% | Val: 86.5% | Time: 438.2s\n",
      "    [DS-CNN-S] Batch 100/1356 Loss: 0.9502 Acc: 85.5%\n",
      "    [DS-CNN-S] Batch 200/1356 Loss: 0.9353 Acc: 86.1%\n",
      "    [DS-CNN-S] Batch 300/1356 Loss: 0.9319 Acc: 86.4%\n",
      "    [DS-CNN-S] Batch 400/1356 Loss: 0.9256 Acc: 86.6%\n",
      "    [DS-CNN-S] Batch 500/1356 Loss: 0.9173 Acc: 87.0%\n",
      "    [DS-CNN-S] Batch 600/1356 Loss: 0.9095 Acc: 87.3%\n",
      "    [DS-CNN-S] Batch 700/1356 Loss: 0.9049 Acc: 87.5%\n",
      "    [DS-CNN-S] Batch 800/1356 Loss: 0.8995 Acc: 87.7%\n",
      "    [DS-CNN-S] Batch 900/1356 Loss: 0.8942 Acc: 88.0%\n",
      "    [DS-CNN-S] Batch 1000/1356 Loss: 0.8897 Acc: 88.2%\n",
      "    [DS-CNN-S] Batch 1100/1356 Loss: 0.8848 Acc: 88.4%\n",
      "    [DS-CNN-S] Batch 1200/1356 Loss: 0.8812 Acc: 88.5%\n",
      "    [DS-CNN-S] Batch 1300/1356 Loss: 0.8771 Acc: 88.7%\n",
      "  Epoch 2/30 | Loss: 0.8752 | Train: 88.8% | Val: 91.3% | Time: 436.8s\n",
      "    [DS-CNN-S] Batch 100/1356 Loss: 0.8180 Acc: 91.5%\n",
      "    [DS-CNN-S] Batch 200/1356 Loss: 0.8212 Acc: 91.2%\n",
      "    [DS-CNN-S] Batch 300/1356 Loss: 0.8181 Acc: 91.3%\n",
      "    [DS-CNN-S] Batch 400/1356 Loss: 0.8164 Acc: 91.4%\n",
      "    [DS-CNN-S] Batch 500/1356 Loss: 0.8151 Acc: 91.4%\n",
      "    [DS-CNN-S] Batch 600/1356 Loss: 0.8128 Acc: 91.5%\n",
      "    [DS-CNN-S] Batch 700/1356 Loss: 0.8118 Acc: 91.5%\n",
      "    [DS-CNN-S] Batch 800/1356 Loss: 0.8100 Acc: 91.6%\n",
      "    [DS-CNN-S] Batch 900/1356 Loss: 0.8088 Acc: 91.6%\n",
      "    [DS-CNN-S] Batch 1000/1356 Loss: 0.8067 Acc: 91.7%\n",
      "    [DS-CNN-S] Batch 1100/1356 Loss: 0.8055 Acc: 91.8%\n",
      "    [DS-CNN-S] Batch 1200/1356 Loss: 0.8041 Acc: 91.8%\n",
      "    [DS-CNN-S] Batch 1300/1356 Loss: 0.8031 Acc: 91.8%\n",
      "  Epoch 3/30 | Loss: 0.8021 | Train: 91.9% | Val: 91.8% | Time: 441.0s\n",
      "    [DS-CNN-S] Batch 100/1356 Loss: 0.7735 Acc: 92.8%\n",
      "    [DS-CNN-S] Batch 200/1356 Loss: 0.7773 Acc: 92.7%\n",
      "    [DS-CNN-S] Batch 300/1356 Loss: 0.7744 Acc: 92.9%\n",
      "    [DS-CNN-S] Batch 400/1356 Loss: 0.7752 Acc: 92.9%\n",
      "    [DS-CNN-S] Batch 500/1356 Loss: 0.7768 Acc: 92.8%\n",
      "    [DS-CNN-S] Batch 600/1356 Loss: 0.7764 Acc: 92.8%\n",
      "    [DS-CNN-S] Batch 700/1356 Loss: 0.7759 Acc: 92.9%\n",
      "    [DS-CNN-S] Batch 800/1356 Loss: 0.7760 Acc: 92.8%\n",
      "    [DS-CNN-S] Batch 900/1356 Loss: 0.7758 Acc: 92.9%\n",
      "    [DS-CNN-S] Batch 1000/1356 Loss: 0.7744 Acc: 92.9%\n",
      "    [DS-CNN-S] Batch 1100/1356 Loss: 0.7731 Acc: 92.9%\n",
      "    [DS-CNN-S] Batch 1200/1356 Loss: 0.7726 Acc: 92.9%\n",
      "    [DS-CNN-S] Batch 1300/1356 Loss: 0.7717 Acc: 92.9%\n",
      "  Epoch 4/30 | Loss: 0.7716 | Train: 92.9% | Val: 94.8% | Time: 439.1s\n",
      "    [DS-CNN-S] Batch 100/1356 Loss: 0.7499 Acc: 93.8%\n",
      "    [DS-CNN-S] Batch 200/1356 Loss: 0.7543 Acc: 93.6%\n",
      "    [DS-CNN-S] Batch 300/1356 Loss: 0.7544 Acc: 93.6%\n",
      "    [DS-CNN-S] Batch 400/1356 Loss: 0.7562 Acc: 93.6%\n",
      "    [DS-CNN-S] Batch 500/1356 Loss: 0.7570 Acc: 93.5%\n",
      "    [DS-CNN-S] Batch 600/1356 Loss: 0.7569 Acc: 93.5%\n",
      "    [DS-CNN-S] Batch 700/1356 Loss: 0.7562 Acc: 93.5%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "#@title Cell 2b: Noise Evaluation Only (학습 완료된 모델만)\n#@markdown ### BC-ResNet-1 학습 완료 후 Cell 2를 Ctrl+C로 중단하고 이 셀 실행\n#@markdown - NanoMamba-Small (95.24%), NanoMamba-Tiny (92.94%)\n#@markdown - DS-CNN-S (96.60%), BC-ResNet-1 (~95.7%)\n#@markdown - 3 noise types × 7 SNR levels = 84 evaluations\n\nimport subprocess, sys, os\nos.chdir('/content/NanoMamba-Interspeech2026')\n\n# Only models that have trained checkpoints\nTRAINED_MODELS = \"NanoMamba-Small,NanoMamba-Tiny,DS-CNN-S,BC-ResNet-1\"\n\ncmd = [\n    sys.executable, \"-u\", \"train_all_models.py\",\n    \"--data_dir\", \"./data\",\n    \"--checkpoint_dir\", \"./checkpoints_full\",\n    \"--eval_only\",\n    \"--models\", TRAINED_MODELS,\n    \"--noise_types\", \"factory,white,babble\",\n    \"--snr_range=-15,-10,-5,0,5,10,15\",\n    \"--per_class\",\n]\n\nprint(f\"Running: {' '.join(cmd)}\\n\")\nprint(\"=\" * 60)\nprint(\"  NOISE EVALUATION - 4 models × 3 noise × 7 SNR\")\nprint(\"  Estimated time: ~30-45 minutes\")\nprint(\"=\" * 60 + \"\\n\")\n\nprocess = subprocess.Popen(\n    cmd,\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n    text=True,\n    bufsize=1\n)\n\nfor line in process.stdout:\n    print(line, end='')\n\nprocess.wait()\n\nif process.returncode == 0:\n    print(\"\\n\" + \"=\" * 60)\n    print(\"  ✅ NOISE EVALUATION COMPLETE!\")\n    print(\"  Results saved to: checkpoints_full/results/\")\n    print(\"=\" * 60)\nelse:\n    print(f\"\\n❌ Process exited with code {process.returncode}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#@title Cell 2c: Latency & MACs 측정 (CPU + GPU)\n#@markdown Hook 기반 MACs 카운팅 + CPU/GPU latency\n#@markdown - NanoMamba: raw audio (1, 16000)\n#@markdown - CNN models: mel spectrogram (1, 40, 98)\n\nimport torch, time, sys, os, json\nimport torch.nn as nn\nos.chdir('/content/NanoMamba-Interspeech2026')\nsys.path.insert(0, '/content/NanoMamba-Interspeech2026')\nfrom train_all_models import create_all_models\n\n# --- MACs counter using forward hooks (no thop needed) ---\ndef count_macs(model, input_tensor):\n    total_macs = [0]\n    hooks = []\n    def linear_hook(m, inp, out):\n        x = inp[0]\n        seq = x.shape[1] if x.dim() == 3 else 1\n        total_macs[0] += seq * m.in_features * m.out_features\n    def conv1d_hook(m, inp, out):\n        total_macs[0] += (m.in_channels // m.groups) * m.kernel_size[0] * m.out_channels * out.shape[2]\n    def conv2d_hook(m, inp, out):\n        total_macs[0] += (m.in_channels // m.groups) * m.kernel_size[0] * m.kernel_size[1] * m.out_channels * out.shape[2] * out.shape[3]\n    for m in model.modules():\n        if isinstance(m, nn.Linear): hooks.append(m.register_forward_hook(linear_hook))\n        elif isinstance(m, nn.Conv1d): hooks.append(m.register_forward_hook(conv1d_hook))\n        elif isinstance(m, nn.Conv2d): hooks.append(m.register_forward_hook(conv2d_hook))\n    with torch.no_grad():\n        model(input_tensor)\n    for h in hooks: h.remove()\n    return total_macs[0]\n\n# --- Input shape per model type ---\ndef get_input(model, device):\n    if hasattr(model, 'snr_estimator'):\n        return torch.randn(1, 16000, device=device)  # NanoMamba: raw audio\n    else:\n        return torch.randn(1, 40, 98, device=device)  # CNN: mel spectrogram\n\nmodels_to_test = ['NanoMamba-Small', 'NanoMamba-Tiny', 'DS-CNN-S', 'BC-ResNet-1']\nall_models = create_all_models()\nresults = {}\n\nhas_gpu = torch.cuda.is_available()\ngpu_name = torch.cuda.get_device_name(0) if has_gpu else 'N/A'\nprint(f\"GPU: {gpu_name}\")\n\nheader = f\"{'Model':<22} {'Params':>8} {'INT8(KB)':>9} {'MACs(M)':>9} {'CPU(ms)':>10}\"\nif has_gpu: header += f\" {'GPU(ms)':>10}\"\nprint(f\"\\n{header}\")\nprint(\"=\" * len(header))\n\nfor name in models_to_test:\n    if name not in all_models:\n        print(f\"{name:<22} NOT FOUND\"); continue\n    model = all_models[name].eval()\n    params = sum(p.numel() for p in model.parameters())\n    int8_kb = params / 1024\n\n    # MACs (CPU)\n    model_cpu = model.to('cpu')\n    inp_cpu = get_input(model_cpu, torch.device('cpu'))\n    macs = count_macs(model_cpu, inp_cpu)\n\n    # CPU Latency\n    with torch.no_grad():\n        for _ in range(50): model_cpu(inp_cpu)\n        t0 = time.time()\n        for _ in range(200): model_cpu(inp_cpu)\n        lat_cpu = (time.time() - t0) / 200 * 1000\n\n    # GPU Latency\n    lat_gpu = None\n    if has_gpu:\n        model_gpu = model.to('cuda')\n        inp_gpu = get_input(model_gpu, torch.device('cuda'))\n        with torch.no_grad():\n            for _ in range(200): model_gpu(inp_gpu)\n            torch.cuda.synchronize()\n            t0 = time.time()\n            for _ in range(1000): model_gpu(inp_gpu)\n            torch.cuda.synchronize()\n            lat_gpu = (time.time() - t0) / 1000 * 1000\n\n    row = f\"{name:<22} {params/1e3:>7.1f}K {int8_kb:>8.1f} {macs/1e6:>8.2f}M {lat_cpu:>9.2f}\"\n    if lat_gpu is not None: row += f\" {lat_gpu:>9.2f}\"\n    print(row)\n\n    results[name] = {\n        'params': int(params), 'int8_kb': round(int8_kb, 1),\n        'macs': int(macs), 'macs_m': round(macs/1e6, 2),\n        'latency_cpu_ms': round(lat_cpu, 2),\n        'latency_gpu_ms': round(lat_gpu, 2) if lat_gpu else None,\n        'gpu': gpu_name\n    }\n\n# Save\nos.makedirs('checkpoints_full/results', exist_ok=True)\nwith open('checkpoints_full/results/efficiency.json', 'w') as f:\n    json.dump(results, f, indent=2)\nprint(f\"\\n✅ Saved to checkpoints_full/results/efficiency.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MN9SLDYuAPs6"
   },
   "source": [
    "#@title Cell 3: 결과 확인 + 다운로드\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Find result files\n",
    "result_files = glob.glob('checkpoints_full/results/*.json')\n",
    "print(f\"Found {len(result_files)} result files:\")\n",
    "for f in sorted(result_files):\n",
    "    print(f\"  {f}\")\n",
    "\n",
    "# Show latest results\n",
    "if result_files:\n",
    "    latest = sorted(result_files)[-1]\n",
    "    with open(latest) as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  Results from: {latest}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Clean accuracy table\n",
    "    if 'model_results' in results:\n",
    "        print(f\"\\n{'Model':<30} {'Params':>8} {'Val':>8} {'Test':>8}\")\n",
    "        print('-' * 58)\n",
    "        for name, data in results['model_results'].items():\n",
    "            val = data.get('best_val_acc', '-')\n",
    "            test = data.get('test_acc', '-')\n",
    "            params = data.get('params', '-')\n",
    "            val_str = f\"{val:.2f}%\" if isinstance(val, (int, float)) else str(val)\n",
    "            test_str = f\"{test:.2f}%\" if isinstance(test, (int, float)) else str(test)\n",
    "            print(f\"{name:<30} {str(params):>8} {val_str:>8} {test_str:>8}\")\n",
    "\n",
    "    # Noise robustness table\n",
    "    if 'noise_results' in results:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"  Noise Robustness Results\")\n",
    "        print(f\"{'='*70}\")\n",
    "        noise_data = results['noise_results']\n",
    "        for model_name, model_noise in noise_data.items():\n",
    "            print(f\"\\n  {model_name}:\")\n",
    "            for noise_type, snr_results in model_noise.items():\n",
    "                snr_str = \", \".join([f\"{snr}dB:{acc:.1f}%\"\n",
    "                                     for snr, acc in sorted(snr_results.items(),\n",
    "                                     key=lambda x: float(x[0]) if x[0] != 'clean' else 999)])\n",
    "                print(f\"    {noise_type}: {snr_str}\")\n",
    "\n",
    "# Zip all results for download\n",
    "!zip -r /content/smartear_results.zip checkpoints_full/\n",
    "\n",
    "from google.colab import files\n",
    "files.download('/content/smartear_results.zip')\n",
    "print(\"\\nResults downloaded!\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}