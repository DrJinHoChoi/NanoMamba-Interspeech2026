% ============================================================
% NanoMamba: IEEE/ACM TASLP Extended References
% ============================================================

% ===========================================
% SSM / Mamba
% ===========================================
@article{gu2024mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2024}
}

@inproceedings{gu2022efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and Re, Christopher},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{gu2020hippo,
  title={{HiPPO}: Recurrent memory with optimal polynomial projections},
  author={Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and Re, Christopher},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{gu2021combining,
  title={Combining recurrent, convolutional, and continuous-time models with linear state-space layers},
  author={Gu, Albert and Johnson, Isys and Goel, Karan and Saab, Khaled and Dao, Tri and Rudra, Atri and Re, Christopher},
  booktitle={NeurIPS},
  year={2021}
}

@article{goel2024keyword,
  title={Keyword {M}amba: Spoken keyword spotting with state space models},
  author={Goel, Rishi and Mousavi, Hojjat and Avan, Viet-Nhat and Subakan, Cem and Bhatt, Geet},
  journal={Speech Communication},
  year={2025},
  publisher={Elsevier}
}

@inproceedings{shaker2024ssamba,
  title={{SSAMBA}: Self-supervised audio representation learning with {M}amba state space model},
  author={Shaker, Siavash and Gong, Yuan and Glass, James},
  booktitle={Proc. Interspeech},
  year={2024}
}

@article{dao2024mamba2,
  title={Transformers are {SSM}s: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert},
  journal={arXiv preprint arXiv:2405.21060},
  year={2024}
}

@inproceedings{smith2023s5,
  title={Simplified state space layers for sequence modeling},
  author={Smith, Jimmy T.H. and Warrington, Andrew and Linderman, Scott W.},
  booktitle={ICLR},
  year={2023}
}

% ===========================================
% Self-citation (Interspeech 2026)
% ===========================================
@inproceedings{choi2026nanomamba_interspeech,
  title={{NanoMamba}: Spectral-Aware State Space Models for Noise-Robust Ultra-Compact Keyword Spotting},
  author={Choi, Jinho},
  booktitle={Proc. Interspeech},
  year={2026},
  note={Submitted}
}

% ===========================================
% KWS Architectures
% ===========================================
@inproceedings{zhang2017hello,
  title={Hello edge: Keyword spotting on microcontrollers},
  author={Zhang, Yundong and Suda, Naveen and Lai, Liangzhen and Chandra, Vikas},
  booktitle={arXiv preprint arXiv:1711.07128},
  year={2017}
}

@inproceedings{kim2021bcresnet,
  title={Broadcasted residual learning for efficient keyword spotting},
  author={Kim, Byeonggeun and Chang, Simyung and Lee, Jinkyu and Sung, Dooyong},
  booktitle={Proc. Interspeech},
  pages={4538--4542},
  year={2021}
}

@inproceedings{choi2019temporal,
  title={Temporal convolution for real-time keyword spotting on mobile devices},
  author={Choi, Seungwoo and Seo, Seokjun and Shin, Beomjun and Byun, Hyeongmin and Kersner, Martin and Kim, Beomsu and Kim, Dongyoung and Ha, Sungjoo},
  booktitle={Proc. Interspeech},
  pages={3372--3376},
  year={2019}
}

@inproceedings{majumdar2020matchboxnet,
  title={{MatchboxNet}: 1D time-channel separable convolutional neural network architecture for speech commands recognition},
  author={Majumdar, Somshubra and Ginsburg, Boris},
  booktitle={Proc. Interspeech},
  pages={3356--3360},
  year={2020}
}

@inproceedings{berg2021keyword,
  title={Keyword transformer: A self-attention model for keyword spotting},
  author={Berg, Axel and O'Connor, Mark and Cruz, Miguel Toglia},
  booktitle={Proc. Interspeech},
  pages={4249--4253},
  year={2021}
}

@inproceedings{deandrade2018neural,
  title={Neural attention models for speech command recognition},
  author={de Andrade, Douglas Coimbra and Leo, Sabato and Viana, Martin Loesener Da Silva and Bernkopf, Christoph},
  booktitle={arXiv preprint arXiv:1808.08929},
  year={2018}
}

@inproceedings{rybakov2020streaming,
  title={Streaming keyword spotting on mobile devices},
  author={Rybakov, Oleg and Kononenko, Natasha and Subrahmanya, Niranjan and Visontai, Mirko and Laurenzo, Stella},
  booktitle={Proc. Interspeech},
  pages={2277--2281},
  year={2020}
}

@inproceedings{bartoli2025typman,
  title={End-to-End Efficiency in Keyword Spotting: A System-Level Approach for Embedded Microcontrollers},
  author={Bartoli, Pietro and Bondini, Tommaso and Veronesi, Christian and Giudici, Andrea and Antonello, Niccol\`{o} and Zappa, Franco},
  booktitle={Proc. IEEE SENSORS},
  year={2025},
  doi={10.1109/SENSORS59705.2025.11330257}
}

% ===========================================
% PCEN / Noise-Robust Front-Ends
% ===========================================
@inproceedings{wang2017trainable,
  title={Trainable frontend for robust and far-field keyword spotting},
  author={Wang, Yuxuan and Getreuer, Pascal and Hughes, Thad and Lyon, Richard F. and Saurous, Rif A.},
  booktitle={Proc. IEEE ICASSP},
  pages={5670--5674},
  year={2017}
}

@inproceedings{zeghidour2021leaf,
  title={{LEAF}: A learnable frontend for audio classification},
  author={Zeghidour, Neil and Teboul, Olivier and de Chaumont Quitry, F\'{e}lix and Tagliasacchi, Marco},
  booktitle={ICLR},
  year={2021}
}

@book{lyon2017human,
  title={Human and Machine Hearing: Extracting Meaning from Sound},
  author={Lyon, Richard F.},
  publisher={Cambridge University Press},
  year={2017}
}

@inproceedings{pcen_lostanlen2019,
  title={Per-channel energy normalization: Why and how},
  author={Lostanlen, Vincent and Salamon, Justin and Cartwright, Mark and McFee, Brian and Farnsworth, Andrew and Kelling, Steve and Bello, Juan Pablo},
  booktitle={IEEE Signal Processing Letters},
  volume={26},
  number={1},
  pages={39--43},
  year={2019}
}

% ===========================================
% Spectral Subtraction / Enhancement
% ===========================================
@article{boll1979suppression,
  title={Suppression of acoustic noise in speech using spectral subtraction},
  author={Boll, Steven},
  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume={27},
  number={2},
  pages={113--120},
  year={1979}
}

@book{haykin2014adaptive,
  title={Adaptive Filter Theory},
  author={Haykin, Simon O.},
  edition={5th},
  publisher={Pearson},
  year={2014}
}

@book{loizou2013speech,
  title={Speech Enhancement: Theory and Practice},
  author={Loizou, Philipos C.},
  publisher={CRC Press},
  edition={2nd},
  year={2013}
}

% ===========================================
% Mixture of Experts / Noise-Conditional
% ===========================================
@inproceedings{zhang2025noisemoe,
  title={Noise-conditioned mixture of experts for robust speaker verification},
  author={Zhang, Shiliang and Wu, Zhifu and Li, Ming},
  booktitle={Proc. IEEE ICASSP},
  year={2025}
}

@article{puigcerver2024moe_robustness,
  title={From sparse to soft mixtures of experts},
  author={Puigcerver, Joan and Riquelme, Carlos and Mustafa, Basil and Houlsby, Neil},
  journal={arXiv preprint arXiv:2308.00951},
  year={2024}
}

@inproceedings{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  booktitle={ICLR},
  year={2017}
}

% ===========================================
% Data Augmentation / Noise Robustness
% ===========================================
@inproceedings{park2019specaugment,
  title={{SpecAugment}: A simple data augmentation method for automatic speech recognition},
  author={Park, Daniel S. and Chan, William and Zhang, Yu and Chiu, Chung-Cheng and Zoph, Barret and Cubuk, Ekin D. and Le, Quoc V.},
  booktitle={Proc. Interspeech},
  pages={2613--2617},
  year={2019}
}

@inproceedings{ko2015audio,
  title={Audio augmentation for speech recognition},
  author={Ko, Tom and Peddinti, Vijayaditya and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={Proc. Interspeech},
  pages={3586--3589},
  year={2015}
}

@article{snyder2015musan,
  title={{MUSAN}: A music, speech, and noise corpus},
  author={Snyder, David and Chen, Guoguo and Povey, Daniel},
  journal={arXiv preprint arXiv:1510.08484},
  year={2015}
}

% ===========================================
% Dataset
% ===========================================
@article{warden2018speech,
  title={Speech commands: A dataset for limited-vocabulary speech recognition},
  author={Warden, Pete},
  journal={arXiv preprint arXiv:1804.03209},
  year={2018}
}

% ===========================================
% Edge / TinyML
% ===========================================
@article{banbury2021mlperf,
  title={{MLPerf} tiny benchmark},
  author={Banbury, Colby and Reddi, Vijay Janapa and Torelli, Peter and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{lin2020mcunet,
  title={{MCUNet}: Tiny deep learning on {IoT} devices},
  author={Lin, Ji and Chen, Wei-Ming and Lin, Yujun and Cohn, John and Gan, Chuang and Han, Song},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{lin2022ondevice,
  title={On-device training under 256{KB} memory},
  author={Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{rusci2020memory,
  title={Memory-driven mixed low precision quantization for enabling deep network inference on microcontrollers},
  author={Rusci, Manuele and Capotondi, Alessandro and Benini, Luca},
  booktitle={Proc. MLSys},
  year={2020}
}

% ===========================================
% Batch Normalization / Normalization
% ===========================================
@inproceedings{ioffe2015batchnorm,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={ICML},
  pages={448--456},
  year={2015}
}

@article{ba2016layernorm,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

% ===========================================
% Optimization
% ===========================================
@inproceedings{loshchilov2019adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={CVPR},
  pages={2818--2826},
  year={2016}
}

@inproceedings{loshchilov2017sgdr,
  title={{SGDR}: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={ICLR},
  year={2017}
}

% ===========================================
% Knowledge Distillation for KWS
% ===========================================
@inproceedings{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  booktitle={NeurIPS Workshop},
  year={2015}
}

@inproceedings{kim2022qbyod,
  title={Compressing keyword spotting models with quantization-aware training and knowledge distillation},
  author={Kim, Byeonggeun and Chang, Simyung},
  booktitle={Proc. Interspeech},
  year={2022}
}

% ===========================================
% Attention / Transformers in Audio
% ===========================================
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NeurIPS},
  year={2017}
}

@inproceedings{gong2021ast,
  title={{AST}: Audio Spectrogram Transformer},
  author={Gong, Yuan and Chung, Yu-An and Glass, James},
  booktitle={Proc. Interspeech},
  pages={571--575},
  year={2021}
}

% ===========================================
% Dynamical Systems / Stability
% ===========================================
@book{khalil2002nonlinear,
  title={Nonlinear Systems},
  author={Khalil, Hassan K.},
  edition={3rd},
  publisher={Prentice Hall},
  year={2002}
}

@inproceedings{orvieto2023resurrecting,
  title={Resurrecting recurrent neural networks for long sequences},
  author={Orvieto, Antonio and Smith, Samuel L. and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
  booktitle={ICML},
  year={2023}
}
